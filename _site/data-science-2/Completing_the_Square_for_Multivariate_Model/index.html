<!DOCTYPE html>
<html>
  <head>
      <title>Completing the Square for Multivariate Normal Model – Jethro Andersson Apriliano Ofe – Forgiven sinner and Student at Maranatha Christian University</title>

        <meta charset="utf-8" />
    <meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>

    
    <meta name="description" content="The subchapter 3.5 of Bayesian Data Analysis Third Edition gives distributional results of Bayesian inference for the parameters of a multivariate normal distribution with a known variance. Additionally, this article discusses the derivation of those results (Equation 3.13 of the book) in gory details.

$\pmb{\text{Figure 1}}$: A posterior distribution equals to a likelihood times a prior divided by a piece of evidence. Image taken from Wikipedia, some rights reserved.
" />
    <meta property="og:description" content="The subchapter 3.5 of Bayesian Data Analysis Third Edition gives distributional results of Bayesian inference for the parameters of a multivariate normal distribution with a known variance. Additionally, this article discusses the derivation of those results (Equation 3.13 of the book) in gory details.

$\pmb{\text{Figure 1}}$: A posterior distribution equals to a likelihood times a prior divided by a piece of evidence. Image taken from Wikipedia, some rights reserved.
" />
    
    <meta name="author" content="Jethro Andersson Apriliano Ofe" />

    
    <meta property="og:title" content="Completing the Square for Multivariate Normal Model" />
    <meta property="twitter:title" content="Completing the Square for Multivariate Normal Model" />
    

    <!--[if lt IE 9]>
      <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <link rel="icon" href="/favicon.png" type="image/gif">

    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="alternate" type="application/rss+xml" title="Jethro Andersson Apriliano Ofe - Forgiven sinner and Student at Maranatha Christian University" href="/feed.xml" />

    <!-- Created with Jekyll Now - http://github.com/barryclark/jekyll-now -->
  </head>

  <body>

    <div class="vertical-center"> 
      <div class="container">
        <div class="row">
              <div id="left-bar">
                
                  <div class="col-lg-offset-1 col-lg-3 col-md-12 col-sm-12 col-xs-12 left-bar border-right">

    
        <a href="http://localhost:4000/">
            <img src="/assets/images/me-gullit-model.jpg" class="img-responsive hidden-xs hidden-sm hidden-md" alt=""/>
        </a>
    
    <h3 class="text-center">Jethro Andersson Apriliano Ofe <small></small></h3>
<!--    <p class="text-center" style="color: #707073;">Forgiven sinner and Student at Maranatha Christian University</p> -->
        <p class="text-center" style="color: #707073;"><a href="https://jethrogit.github.io/sermons/Forgive_Sinner/"><b>A Forgiven Sinner</b></a> and <a href="https://www.maranatha.edu/"><b>Student at Maranatha Christian University</b></a></p> 
    <div class="hidden-lg" style="margin-bottom:5px;">
        <p class="text-center">
            <a href="http://localhost:4000" class="color-icon" title="Home"><i class="fa fa-home" aria-hidden="true"></i></a> - 
            
<a href="mailto:jethro.andersson02@gmail.com" class="color-icon"><i class="fa fa-envelope" title="Email"></i></a>


<a href="https://github.com/jethroGIT" class="color-icon" title="Github"><i class="fa fa-github"></i></a>

<a href="https://www.linkedin.com/in/jethro-andersson-apriliano-ofe" class="color-icon" title="Linkedin"><i class="fa fa-linkedin"></i></a>



<!-- <a href="http://stackoverflow.com/users/1473726" class="color-icon" title="StacKoverflow"><i class="fa fa-stack-overflow"></i></a> -->




        </p>
    </div>
    <div class="hidden-xs hidden-sm hidden-md" style="margin-bottom:5px;">
        <hr >

        
        <i class="fa fa-github fw"></i><a href="https://github.com/jethroGIT"> github.com/jethroGIT</a>    <br />
         

        

        

        
        <i class="fa fa-envelope fw"></i><a href="jethro.andersson02@gmail.com" target="_blank"> jethro.andersson02@gmail.com</a>
        

        

        <br>

        
        <i class="fa fa-linkedin fw"></i><a href="https://www.linkedin.com/in/jethro-andersson-apriliano-ofe" target="_blank"> jethro-andersson-apriliano-ofe</a>
        

        

        

        

<!--        
        <hr >
        <a href="https://stackoverflow.com/users/1473726?tab=profile">
           <img src="https://stackoverflow.com/users/flair/1473726.png?theme=clean" width="208" height="58" alt="profile for  at Stack Overflow, Q&amp;A for professional and enthusiast programmers" title="profile for  at Stack Overflow, Q&amp;A for professional and enthusiast programmers">
        </a>
        
        <hr >
        
        <p>You may also want to check this profile:
            <ul>
                
                  <li><a href=""></a></li>
                
            </ul>
        </p>
        
-->        
    </div>

</div>

                
              </div>
              <div class=" col-lg-8   col-sm-12 col-xs-12">
  <div class="row grid-content"> 
      <div class="col-md-12 col-xs-12 projet" style="margin-bottom:20px;">
          <p class="hidden-xs hidden-sm"><a href="http://localhost:4000/">Home</a> <span style="color:darkgrey"> / <a href="http://localhost:4000/data-science-2">data-science-2</a> / completing the square for multivariate normal model</span></p>
          <h3>Completing the Square for Multivariate Normal Model<small class="pull-right"><a href="/feed.xml" target="_blank"><i class="fa fa-rss-square rss"></i></a></small><small class="pull-right"><a class="twitter-share-button" href="https://twitter.com/intent/tweet?via=&text=Equation (3.13) from BDA on page 71">Tweet</a></small></h3> 
          <!-- <hr>
              <p class="text-center">
               Completing the Square for Multivariate Normal Model <br> 
                  <span class="text-small">Equation (3.13) from BDA on page 71</span> <br>
                  

              </p>
              <hr> -->

            <div class="entry">
              <p>The subchapter 3.5 of <a href="http://www.stat.columbia.edu/~gelman/book/BDA3.pdf"><strong>Bayesian Data Analysis Third Edition</strong></a> gives distributional results of Bayesian inference for the parameters of a multivariate normal distribution with a <strong>known</strong> variance. <em>Additionally, this article discusses the derivation of those results (Equation 3.13 of the book) in <strong>gory details</strong>.</em></p>

<p><a href="/assets/images/bayes-theorem.jpg"><img src="/assets/images/bayes-theorem.jpg" alt="img1" class="img-responsive" /></a><em><center>$\pmb{\text{Figure 1}}$: A posterior distribution equals to a likelihood times a prior divided by a piece of evidence. Image taken from <a href="https://en.wikipedia.org/wiki/Bayes%27_theorem">Wikipedia</a>, some rights reserved.</center></em></p>

<p>Suppose we have a model for an observable vector $y$ of $d$ components, that is $y$ is a column vector of $d \times 1$, with the multivariate normal distribution,</p>

\[\begin{equation}
    y \mid \mu, \Sigma \sim \text{N}(\mu, \Sigma) \tag{1}\label{eq:mvn-one-sample} 
\end{equation}\]

<p>where $\mu$ is a column vector of length $d$ and $\Sigma$ is a known $d \times d$ variance matrix, which is <a href="https://en.wikipedia.org/wiki/Symmetric_matrix"><em>symmetric</em></a> and <a href="https://en.wikipedia.org/wiki/Definite_matrix"><em>positive definite</em></a>. Therefore, the <em>likelihood function</em> for a single observation is</p>

\[\begin{equation}
    \Pr(y \mid \mu, \Sigma) \propto \lvert \Sigma \rvert^{-1/2} \exp \left( - \frac{1}{2} (y-\mu)^T \Sigma^{-1} (y - \mu) \right),  \tag{2}\label{eq:likelihood-one-sample} 
\end{equation}\]

<p>and for a sample of $n$ independent and identically distributed observations, $y_1, \ldots, y_n$, is</p>

\[\begin{align}
   \Pr( y_1, \ldots, y_n \mid \mu, \Sigma ) &amp;\propto \prod_{i=1}^{n}{ \Pr( y_i \mid \mu, \Sigma ) }     \tag{3}\label{eq:likelihood-samples-1}  \\
   &amp;= \prod_{i=1}^{n}{ \lvert \Sigma \rvert^{-1/2} \exp \left( - \frac{1}{2} (y_i-\mu)^T \Sigma^{-1} (y_i - \mu) \right) }     \tag{4}\label{eq:likelihood-samples-2} &amp;&amp; \text{using Equation }\eqref{eq:likelihood-one-sample} \\
   &amp;= \prod_{i=1}^{n}{ \lvert \Sigma \rvert^{-1/2} } \prod_{i=1}^{n}{\exp \left( - \frac{1}{2} (y_i-\mu)^T \Sigma^{-1} (y_i - \mu) \right) }     \tag{5}\label{eq:likelihood-samples-3}  \\  
   &amp;= \lvert \Sigma \rvert^{-n/2} \exp \left( - \frac{1}{2} \sum_{i=1}^{n}{(y_i-\mu)^T \Sigma^{-1} (y_i - \mu)} \right).  \tag{6}\label{eq:likelihood-samples-4}  \\     
\end{align}\]

<p>Actually, given the following nice <a href="https://en.wikipedia.org/wiki/Trace_(linear_algebra)"><em>trace property</em></a>,</p>

\[\begin{equation}
    \sum_{i=1}^{n}{x_i^T A x_i} = \text{tr}\left( A \sum_{i=1}^{n}{x_i x_i^T} \right) \tag{7}\label{eq:trace-property}
\end{equation}\]

<p>with $x_i$ is a column vector whose dimension is $d \times 1$, $A$ is a symmetric matrix whose dimension is $d \times d$, and $\text{tr}$ is a <a href="https://en.wikipedia.org/wiki/Trace_(linear_algebra)"><em>trace function</em></a>, we can rewrite Equation \eqref{eq:trace-property} as follows:</p>

\[\begin{equation}
\Pr( y_1, \ldots, y_n \mid \mu, \Sigma ) \propto \lvert \Sigma \rvert^{-n/2} \exp \left( -\frac{1}{2} \text{tr}(\Sigma^{-1} S_0) \right) \tag{8}\label{eq:likelihood-final-version}
\end{equation}\]

<p>where $S_0$ is the “<em>sums squares</em>” matrix relative to $\mu$,</p>

\[\begin{equation}
    S_0 = \sum_{i=1}^{n}{(y_i - \mu)(y_i - \mu)^T}. \tag{9}\label{eq:sum-of-squares}
\end{equation}\]

<p>Before we construct the posterior distribution of the model, let’s define the <em>prior distribution</em> as follows:</p>

\[\begin{equation}
\Pr( \mu ) \propto  \lvert \Lambda_0 \rvert^{-1/2} \exp \left(-\frac{1}{2} (\mu - \mu_0)^T \Lambda_0^{-1} (\mu - \mu_0)  \right) \tag{10}\label{eq:prior}
\end{equation}\]

<p>that is $\mu \sim \text{N}(\mu_0, \Lambda_0)$. By the way, $\Lambda_0$ is also a symmetric and positive definite matrix as well.</p>

<blockquote>
  <p>Now that we have both <em>likelihood</em> and <em>prior</em> distributions; let’s compute the posterior distribution of the model,</p>
</blockquote>

\[\begin{align}
\Pr( \mu \mid y, \Sigma ) &amp;\propto \Pr( y \mid \mu, \Sigma ) \Pr(\mu \mid \Sigma) &amp;&amp; \text{by Bayes rule} \tag{11}\label{eq:posterior-def}  \\
                          &amp;= \lvert \Sigma \rvert^{-n/2} \exp \left( - \frac{1}{2} \sum_{i=1}^{n}{(y_i-\mu)^T \Sigma^{-1} (y_i - \mu)} \right) \times \lvert \Lambda_0 \rvert^{-1/2} \exp \left(-\frac{1}{2} (\mu - \mu_0)^T \Lambda_0^{-1} (\mu - \mu_0)  \right)  \\
                          &amp;\propto \exp \left( -\frac{1}{2} \underbrace{ \left( (\mu - \mu_0)^T \Lambda_0^{-1} (\mu - \mu_0) + \sum_{i=1}^{n}{(y_i-\mu)^T \Sigma^{-1} (y_i - \mu)} \right)}_{\text{A}}   \right) \tag{12}\label{eq:posterior-1}
\end{align}\]

<blockquote>
  <p>Part $\text{A}$ in Equation \eqref{eq:posterior-1} is actually a “<em>completing the quadratic form</em>” problem.</p>
</blockquote>

<p>Let’s solve the problem as follows:</p>

\[\begin{align}
\text{A} &amp;= (\mu^T - \mu_0^T) \Lambda_0^{-1} (\mu - \mu_0)  + \sum_{i=1}^{n}{(y_i^T - \mu^T)\Sigma^{-1}(y_i - \mu)} &amp;&amp; \text{by transpose property} \tag{13}\label{eq:complete-squares-1}  \\
&amp;= \underbrace{(\mu^T \Lambda_0^{-1} - \mu_0^T \Lambda_0^{-1}) (\mu - \mu_0)}_{\text{B}}  + \underbrace{\sum_{i=1}^{n}{(y_i^T \Sigma^{-1} - \mu^T \Sigma^{-1})(y_i - \mu)}}_{\text{C}} \tag{14}\label{eq:complete-squares-2} 
\end{align}\]

<p>Let’s multiply out all terms in part $\text{B}$ in Equation \eqref{eq:complete-squares-2} as follows:</p>

\[\begin{align}
    \text{B} &amp;= \mu^T \Lambda_0^{-1} \mu - \underbrace{\mu^T \Lambda_0^{-1} \mu_0}_{\text{a scalar}} - \underbrace{\mu_0^T \Lambda_0^{-1} \mu}_{\text{a scalar}} + \mu_0^T \Lambda_0^{-1} \mu_0  \tag{15}\label{eq:b-1} \\
             &amp;= \mu^T \Lambda_0^{-1} \mu - \mu^T \Lambda_0^{-1} \mu_0 - (\mu^T \Lambda_0^{-1} \mu_0)^T + \mu_0^T \Lambda_0^{-1} \mu_0  &amp;&amp; \text{by transpose property} \tag{16}\label{eq:b-2} \\
             &amp;= \mu^T \Lambda_0^{-1} \mu - \mu^T \Lambda_0^{-1} \mu_0 - \mu^T \Lambda_0^{-1} \mu_0 + \mu_0^T \Lambda_0^{-1} \mu_0 &amp;&amp; \text{as } \mu^T \Lambda_0^{-1} \mu_0 = (\mu^T \Lambda_0^{-1} \mu_0)^T  \tag{17}\label{eq:b-3}  \\             
             &amp;= \mu^T \Lambda_0^{-1} \mu - 2 \mu^T \Lambda_0^{-1} \mu_0 + \mu_0^T \Lambda_0^{-1} \mu_0. \tag{18}\label{eq:b-4}  \\             
\end{align}\]

<p>Let’s also multiply out part $\text{C}$ in Equation \eqref{eq:complete-squares-2},</p>

\[\begin{align}
    \text{C} &amp;= \sum_{i=1}^{n}{(y_i^T \Sigma^{-1} y_i - \underbrace{y_i^T \Sigma^{-1} \mu}_{\text{scalar}} - \underbrace{\mu^T \Sigma^{-1} y_i}_{\text{scalar}} + \mu^T \Sigma^{-1} \mu)} \tag{19}\label{eq:c-1} \\
    &amp;= \sum_{i=1}^{n}{(y_i^T \Sigma^{-1} y_i - (\mu^T \Sigma^{-1} y_i)^T - \mu^T \Sigma^{-1} y_i + \mu^T \Sigma^{-1} \mu)} &amp;&amp; \text{by transpose property} \tag{20}\label{eq:c-2} \\    
    &amp;= \sum_{i=1}^{n}{(y_i^T \Sigma^{-1} y_i - \mu^T \Sigma^{-1} y_i - \mu^T \Sigma^{-1} y_i + \mu^T \Sigma^{-1} \mu)} &amp;&amp; \text{as } (\mu^T \Sigma^{-1} y_i)^T = \mu^T \Sigma^{-1} y_i \tag{21}\label{eq:c-3} \\        
    &amp;= \sum_{i=1}^{n}{(y_i^T \Sigma^{-1} y_i - 2 \mu^T \Sigma^{-1} y_i + \mu^T \Sigma^{-1} \mu)}  \tag{22}\label{eq:c-4} \\
    &amp;= \sum_{i=1}^{n}{y_i^T \Sigma^{-1} y_i} - \sum_{i=1}^{n}{2 \mu^T \Sigma^{-1} y_i}  + \sum_{i=1}^{n}{\mu^T \Sigma^{-1} \mu} &amp;&amp; \text{by a linear operator of }\sum \tag{23}\label{eq:c-5} \\
    &amp;= \sum_{i=1}^{n}{y_i^T \Sigma^{-1} y_i} - 2 \mu^T \Sigma^{-1} \sum_{i=1}^{n}{y_i}  + \sum_{i=1}^{n}{\mu^T \Sigma^{-1} \mu} \tag{24}\label{eq:c-6}\\
    &amp;= \sum_{i=1}^{n}{y_i^T \Sigma^{-1} y_i} - 2 \mu^T \Sigma^{-1} n \overline{y}  + \sum_{i=1}^{n}{\mu^T \Sigma^{-1} \mu}  &amp;&amp; \text{as }\overline{y} = \frac{\sum_{i=1}^n y_i}{n} \tag{25}\label{eq:c-7} \\
    &amp;= \sum_{i=1}^{n}{y_i^T \Sigma^{-1} y_i} - 2 \mu^T \Sigma^{-1} n \overline{y}  + n \mu^T \Sigma^{-1} \mu  &amp;&amp; \text{as }\sum_{i=1}^{n}{\text{constant}} = n \times \text{constant} \tag{26}\label{eq:c-8} \\    
    &amp;= \sum_{i=1}^{n}{y_i^T \Sigma^{-1} y_i} - 2 \mu^T n \Sigma^{-1}  \overline{y}  +  \mu^T n \Sigma^{-1} \mu  \tag{27}\label{eq:c-9}      \\
    &amp;= \mu^T n \Sigma^{-1} \mu - 2 \mu^T n \Sigma^{-1}  \overline{y} + \sum_{i=1}^{n}{y_i^T \Sigma^{-1} y_i} &amp;&amp; \text{just rearrange terms}   \tag{28}\label{eq:c-10}          
\end{align}\]

<p>Now let’s combine both part $\text{B}$ (Equation \eqref{eq:b-4}) and part $\text{C}$ (Equation \eqref{eq:c-10}) into part $\text{A}$ in Equation \eqref{eq:complete-squares-2},</p>

\[\begin{align}
\text{A} =&amp; \mu^T \Lambda_0^{-1} \mu - 2 \mu^T \Lambda_0^{-1} \mu_0 + \mu_0^T \Lambda_0^{-1} \mu_0 + \\
          &amp; \mu^T n \Sigma^{-1} \mu - 2 \mu^T n \Sigma^{-1}  \overline{y} + \sum_{i=1}^{n}{y_i^T \Sigma^{-1} y_i} \tag{29}\label{eq:c-11} \\
         =&amp; \mu^T (\Lambda_0^{-1} + n \Sigma^{-1} ) \mu - 2 \mu^T ( \Lambda_0^{-1} \mu_0 + n \Sigma^{-1} \overline{y} ) + \underbrace{\mu_0^T \Lambda_0^{-1} \mu_0 + \sum_{i=1}^{n}{y_i^T \Sigma^{-1} y_i}}_{\text{constant}_1}  &amp;&amp; \text{sum all terms accordingly} \tag{30}\label{eq:c-12} \\
         =&amp; \mu^T (\Lambda_0^{-1} + n \Sigma^{-1} ) \mu - 2 \mu^T ( \Lambda_0^{-1} \mu_0 + n \Sigma^{-1} \overline{y} ) + \text{constant}_1  \tag{31}\label{eq:c-13} \\         
         =&amp; \mu^T (\Lambda_0^{-1} + n \Sigma^{-1} ) \mu - \underbrace{\mu^T ( \Lambda_0^{-1} \mu_0 + n \Sigma^{-1} \overline{y} )}_{\text{scalar}} - \underbrace{\mu^T ( \Lambda_0^{-1} \mu_0 + n \Sigma^{-1} \overline{y} )}_{\text{scalar}} + \text{constant}_1 &amp;&amp; \text{separate the middle term}  \tag{32}\label{eq:c-14} \\         
         =&amp; \mu^T (\Lambda_0^{-1} + n \Sigma^{-1} ) \mu - \mu^T ( \Lambda_0^{-1} \mu_0 + n \Sigma^{-1} \overline{y} ) - ( \Lambda_0^{-1} \mu_0 + n \Sigma^{-1} \overline{y} )^T \mu  + \text{constant}_1 &amp;&amp; \text{by transpose property}  \tag{33}\label{eq:c-15} \\    
         =&amp; \left( \mu^T (\Lambda_0^{-1} + n \Sigma^{-1} ) - (\Lambda_0^{-1} \mu_0 + n \Sigma^{-1} \overline{y})^T \right) \left( \mu - (\Lambda_0^{-1} + n \Sigma^{-1})^{-1} (\Lambda_0^{-1} \mu_0 + n \Sigma^{-1} \overline{y}) \right)  \underbrace{- (\Lambda_0^{-1} \mu_0 + n \Sigma^{-1} \overline{y})^T (\Lambda_0^{-1} + n \Sigma^{-1})^{-1} (\Lambda_0^{-1} \mu_0 + n \Sigma^{-1} \overline{y}) + \text{constant}_1 }_{\text{constant}_2}  &amp;&amp; \text{by factoring &amp; inverse matrix}  \tag{34}\label{eq:c-16} \\             
         =&amp; \left( \mu^T (\Lambda_0^{-1} + n \Sigma^{-1} ) - (\Lambda_0^{-1} \mu_0 + n \Sigma^{-1} \overline{y})^T \right) \left( \mu - (\Lambda_0^{-1} + n \Sigma^{-1})^{-1} (\Lambda_0^{-1} \mu_0 + n \Sigma^{-1} \overline{y}) \right) + \text{constant}_2    \tag{35}\label{eq:c-17} \\                      
         =&amp; \left( \mu^T  - (\Lambda_0^{-1} \mu_0 + n \Sigma^{-1} \overline{y})^T (\Lambda_0^{-1} + n \Sigma^{-1} )^{-1} \right) (\Lambda_0^{-1} + n \Sigma^{-1} ) \left( \mu - (\Lambda_0^{-1} + n \Sigma^{-1})^{-1} (\Lambda_0^{-1} \mu_0 + n \Sigma^{-1} \overline{y}) \right) + \text{constant}_2  &amp;&amp; \text{get }(\Lambda_0^{-1} + n \Sigma^{-1} ) \text{ out}  \tag{36}\label{eq:c-18} \\                      
         =&amp; \left( \mu^T  - (\Lambda_0^{-1} \mu_0 + n \Sigma^{-1} \overline{y})^T (\Lambda_0^{-1} + n \Sigma^{-1} )^{-T} \right) (\Lambda_0^{-1} + n \Sigma^{-1} ) \left( \mu - (\Lambda_0^{-1} + n \Sigma^{-1})^{-1} (\Lambda_0^{-1} \mu_0 + n \Sigma^{-1} \overline{y}) \right) + \text{constant}_2  &amp;&amp; \text{symmetric property, }A^{-T} = A^{-1}  \tag{37}\label{eq:c-19} \\                               
         =&amp; \left( \mu  - (\Lambda_0^{-1} + n \Sigma^{-1} )^{-1} (\Lambda_0^{-1} \mu_0 + n \Sigma^{-1} \overline{y})  \right)^T (\Lambda_0^{-1} + n \Sigma^{-1} ) \left( \mu - (\Lambda_0^{-1} + n \Sigma^{-1})^{-1} (\Lambda_0^{-1} \mu_0 + n \Sigma^{-1} \overline{y}) \right) + \text{constant}_2  &amp;&amp; \text{by transpose property}  \tag{38}\label{eq:c-20} \\        
         =&amp; \left( \mu  - \mu_n  \right)^T \Lambda_n^{-1} \left( \mu - \mu_n \right) + \text{constant}_2    \tag{39}\label{eq:c-21} \\                                  
\end{align}\]

<p>where</p>

\[\begin{align}
\mu_n &amp;= (\Lambda_0^{-1} + n \Sigma^{-1} )^{-1} (\Lambda_0^{-1} \mu_0 + n \Sigma^{-1} \overline{y}) \text{ and} \nonumber \\
\Lambda_n^{-1} &amp;= \Lambda_0^{-1} + n \Sigma^{-1}. \tag{40}\label{eq:final-mean-variance} 
\end{align}\]

<p>Next, let’s substitute the part $\text{A}$ (Equation \eqref{eq:c-21}) into the posterior distribution (Equation \eqref{eq:posterior-1}),</p>

\[\begin{align}
\Pr( \mu \mid y, \Sigma ) &amp;\propto \exp \left( -\frac{1}{2} \underbrace{ \left( (\mu - \mu_0)^T \Lambda_0^{-1} (\mu - \mu_0) + \sum_{i=1}^{n}{(y_i-\mu)^T \Sigma^{-1} (y_i - \mu)} \right)}_{\text{A}}   \right) \tag{41}\label{eq:posterior-2} \\
&amp;= \exp \left( -\frac{1}{2} \left( \left( \mu  - \mu_n  \right)^T \Lambda_n^{-1} \left( \mu - \mu_n \right)  + \text{constant}_2 \right) \right) \tag{42}\label{eq:posterior-3} \\
&amp;= \exp \left( -\frac{1}{2}  \left( \mu  - \mu_n  \right)^T \Lambda_n^{-1} \left( \mu - \mu_n \right)   \right) \times \exp \left( \text{constant}_2 \right) \tag{43}\label{eq:posterior-4} \\
&amp;\propto  \exp \left( -\frac{1}{2}  \left( \mu  - \mu_n  \right)^T \Lambda_n^{-1} \left( \mu - \mu_n \right)   \right) \tag{44}\label{eq:posterior-5} \\
&amp;= \text{N}(\mu \mid \mu_n, \Lambda_n) \tag{45}\label{eq:posterior-6}
\end{align}\]

<p>where</p>

\[\begin{align}
\mu_n &amp;= (\Lambda_0^{-1} + n \Sigma^{-1} )^{-1} (\Lambda_0^{-1} \mu_0 + n \Sigma^{-1} \overline{y})  \nonumber \\
\Lambda_n^{-1} &amp;= \Lambda_0^{-1} + n \Sigma^{-1}. \nonumber
\end{align}\]

<p>By the way, the above derivation is also mentioned as <em>Exercise 3.13</em> in <a href="http://www.stat.columbia.edu/~gelman/book/BDA3.pdf"><em>the book</em></a>.</p>

            </div>
            <hr>
            <div class="date">
              Written on June 26, 2021
            </div>
            <br>
            
            
            
<div class="comments">
	<div id="disqus_thread"></div>
	<script type="text/javascript">

	    var disqus_shortname = 'hbunyamin-github-io';

	    (function() {
	        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
	        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	    })();

	</script>
	<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>


      </div>
  </div>


        </div>
      </div>
    </div>
    <script src="/assets/twitter.js"></script>
    
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript" async
  src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

    
	<!-- Google Analytics -->
	<script>
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

		ga('create', 'UA-27831864-1', 'auto');
		ga('send', 'pageview', {
		  'page': '/data-science-2/Completing_the_Square_for_Multivariate_Model/',
		  'title': 'Completing the Square for Multivariate Normal Model'
		});
	</script>
	<!-- End Google Analytics -->


  </body>
</html>
