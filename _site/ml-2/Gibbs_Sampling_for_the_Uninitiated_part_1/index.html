<!DOCTYPE html>
<html>
  <head>
      <title>Gibbs Sampling for the Uninitiated part 1 – Jethro Andersson Apriliano Ofe – Forgiven sinner and Student at Maranatha Christian University</title>

        <meta charset="utf-8" />
    <meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>

    
    <meta name="description" content="This article is inspired by the masterpiece of Gibbs Sampling tutorial by Resnik and Hardisty and also an awesome github repo by Bob Flagg.    Both of these resources are excellent and highly recommended for anyone to read.
" />
    <meta property="og:description" content="This article is inspired by the masterpiece of Gibbs Sampling tutorial by Resnik and Hardisty and also an awesome github repo by Bob Flagg.    Both of these resources are excellent and highly recommended for anyone to read.
" />
    
    <meta name="author" content="Jethro Andersson Apriliano Ofe" />

    
    <meta property="og:title" content="Gibbs Sampling for the Uninitiated part 1" />
    <meta property="twitter:title" content="Gibbs Sampling for the Uninitiated part 1" />
    

    <!--[if lt IE 9]>
      <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <link rel="icon" href="/favicon.png" type="image/gif">

    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="alternate" type="application/rss+xml" title="Jethro Andersson Apriliano Ofe - Forgiven sinner and Student at Maranatha Christian University" href="/feed.xml" />

    <!-- Created with Jekyll Now - http://github.com/barryclark/jekyll-now -->
  </head>

  <body>

    <div class="vertical-center"> 
      <div class="container">
        <div class="row">
              <div id="left-bar">
                
                  <div class="col-lg-offset-1 col-lg-3 col-md-12 col-sm-12 col-xs-12 left-bar border-right">

    
        <a href="http://localhost:4000/">
            <img src="/assets/images/me-gullit-model.jpg" class="img-responsive hidden-xs hidden-sm hidden-md" alt=""/>
        </a>
    
    <h3 class="text-center">Jethro Andersson Apriliano Ofe <small></small></h3>
<!--    <p class="text-center" style="color: #707073;">Forgiven sinner and Student at Maranatha Christian University</p> -->
        <p class="text-center" style="color: #707073;"><a href="https://jethrogit.github.io/sermons/Forgive_Sinner/"><b>A Forgiven Sinner</b></a> and <a href="https://www.maranatha.edu/"><b>Student at Maranatha Christian University</b></a></p> 
    <div class="hidden-lg" style="margin-bottom:5px;">
        <p class="text-center">
            <a href="http://localhost:4000" class="color-icon" title="Home"><i class="fa fa-home" aria-hidden="true"></i></a> - 
            
<a href="mailto:jethro.andersson02@gmail.com" class="color-icon"><i class="fa fa-envelope" title="Email"></i></a>


<a href="https://github.com/jethroGIT" class="color-icon" title="Github"><i class="fa fa-github"></i></a>

<a href="https://www.linkedin.com/in/jethro-andersson-apriliano-ofe" class="color-icon" title="Linkedin"><i class="fa fa-linkedin"></i></a>



<!-- <a href="http://stackoverflow.com/users/1473726" class="color-icon" title="StacKoverflow"><i class="fa fa-stack-overflow"></i></a> -->




        </p>
    </div>
    <div class="hidden-xs hidden-sm hidden-md" style="margin-bottom:5px;">
        <hr >

        
        <i class="fa fa-github fw"></i><a href="https://github.com/jethroGIT"> github.com/jethroGIT</a>    <br />
         

        

        

        
        <i class="fa fa-envelope fw"></i><a href="jethro.andersson02@gmail.com" target="_blank"> jethro.andersson02@gmail.com</a>
        

        

        <br>

        
        <i class="fa fa-linkedin fw"></i><a href="https://www.linkedin.com/in/jethro-andersson-apriliano-ofe" target="_blank"> jethro-andersson-apriliano-ofe</a>
        

        

        

        

<!--        
        <hr >
        <a href="https://stackoverflow.com/users/1473726?tab=profile">
           <img src="https://stackoverflow.com/users/flair/1473726.png?theme=clean" width="208" height="58" alt="profile for  at Stack Overflow, Q&amp;A for professional and enthusiast programmers" title="profile for  at Stack Overflow, Q&amp;A for professional and enthusiast programmers">
        </a>
        
        <hr >
        
        <p>You may also want to check this profile:
            <ul>
                
                  <li><a href=""></a></li>
                
            </ul>
        </p>
        
-->        
    </div>

</div>

                
              </div>
              <div class=" col-lg-8   col-sm-12 col-xs-12">
  <div class="row grid-content"> 
      <div class="col-md-12 col-xs-12 projet" style="margin-bottom:20px;">
          <p class="hidden-xs hidden-sm"><a href="http://localhost:4000/">Home</a> <span style="color:darkgrey"> / <a href="http://localhost:4000/ml-2">ml-2</a> / gibbs sampling for the uninitiated part 1</span></p>
          <h3>Gibbs Sampling for the Uninitiated part 1<small class="pull-right"><a href="/feed.xml" target="_blank"><i class="fa fa-rss-square rss"></i></a></small><small class="pull-right"><a class="twitter-share-button" href="https://twitter.com/intent/tweet?via=&text=Gibbs sampling for dummies">Tweet</a></small></h3> 
          <!-- <hr>
              <p class="text-center">
               Gibbs Sampling for the Uninitiated part 1 <br> 
                  <span class="text-small">Gibbs sampling for dummies</span> <br>
                  

              </p>
              <hr> -->

            <div class="entry">
              <p>This article is inspired by the <em>masterpiece</em> of <a href="https://maranathaedu-my.sharepoint.com/:b:/g/personal/hendra_bunyamin_it_maranatha_edu/EZA9beLgBdZPip9br-UrdSAB024P8IBGESYAbJP3MfRQFQ?e=HdMzZ9"><strong>Gibbs Sampling tutorial</strong></a> by <em>Resnik and Hardisty</em> and also an <em>awesome</em> <a href="https://github.com/bobflagg/gibbs-sampling-for-the-uninitiated"><strong>github repo</strong></a> by <em>Bob Flagg</em>.    Both of these resources are excellent and highly recommended for anyone to read.</p>

<p>This article will show a <em>step by step implementation of a Gibbs sampler</em> for a <strong>Naive Bayes</strong> model in Python.</p>

<p>Let’s start with the problem definition. Assume that we would like to classify whether or not the sentiment of a document is either $0$ (negative) or $1$ (positive) visualized by the following image.</p>

<p><a href="/assets/images/sentiment-positive-negative.jpeg"><img src="/assets/images/sentiment-positive-negative.jpeg" alt="img1" class="img-responsive" /></a><em><center>An illustration of positive and negative sentiments. Image taken from <a href="https://towardsdatascience.com/cnn-sentiment-analysis-1d16b7c5a0e7">Saad Arshad</a>, some rights reserved.</center></em></p>

<p>Moreover, the generative story of how the documents are generated as explained in $\S$2.1 of the paper is shown in the following image.</p>

<p><a href="/assets/images/naive-bayes-graphical-model.png"><img src="/assets/images/naive-bayes-graphical-model.png" alt="img1" class="img-responsive" /></a><em><center>The graphical model of the simple Naive Bayes model.</center></em></p>

<p>Let’s recall some of the notations from the paper. There are $6$ variables in the image and we shall discuss these variables one by one.</p>

<p>$\pmb{\gamma_\pi}$ is a <em>vectorized version of hyperparameters</em> from a <strong>Beta</strong> distribution. In the literature <a href="https://www.amazon.com/Bayesian-Analysis-Chapman-Statistical-Science/dp/1439840954/ref=sr_1_1?crid=64FB8G3F3C3L&amp;dchild=1&amp;keywords=bayesian+data+analysis&amp;qid=1593868386&amp;s=books&amp;sprefix=bayesian+data%2Caps%2C395&amp;sr=1-1">(Gelman et al., 2013)</a>, these hyperparameters usually are represented by $\alpha$ and $\beta$. In the paper, they are $\gamma_{\pi0}$ and $\gamma_{\pi1}$. Specifically, $\pmb{\gamma_\pi}$ is defined as follows:      <br />
\(\begin{equation}
    \pmb{\gamma_\pi} = \begin{bmatrix} \gamma_{\pi0} \\ \gamma_{\pi1} \end{bmatrix}.
\end{equation} \tag{1}\label{eq:gamma-pi}\)</p>

<p>Secondly, $\pi$ is a random variable which has a <strong>Beta</strong> distribution, in other words</p>

\[\begin{equation}
    \pi \sim \text{Beta}(\gamma_{\pi0}, \gamma_{\pi1}) \tag{2}\label{eq:beta}. 
\end{equation}\]

<p>Thirdly, $L_j$ is a random variable for $j$th document which has a <strong>Bernoulli</strong> distribution,</p>

\[\begin{equation}
    L_j \sim \text{Bernoulli}(\pi) \tag{3}\label{eq:binomial}. 
\end{equation}\]

<p>Fourthly, $\pmb{\gamma_{\theta}}$ is a hyperparameter vector whose dimension is <strong>the size of vocabulary</strong> ($V$) and provided for a <strong>Dirichlet</strong> distribution. In the literature <a href="https://www.amazon.com/Bayesian-Analysis-Chapman-Statistical-Science/dp/1439840954/ref=sr_1_1?crid=64FB8G3F3C3L&amp;dchild=1&amp;keywords=bayesian+data+analysis&amp;qid=1593868386&amp;s=books&amp;sprefix=bayesian+data%2Caps%2C395&amp;sr=1-1">(Gelman et al., 2013)</a>, these hyperparameters usually are represented by $\alpha_1$, $\alpha_2$, $\ldots$, $\alpha_V$. In the paper, it is defined as a vector defined as follows:</p>

\[\begin{equation}
    \pmb{\gamma_{\theta}} = \begin{bmatrix} 
            \gamma_{\theta1} \\
            \gamma_{\theta2} \\
            \vdots \\
            \gamma_{\theta V}
        \end{bmatrix} = \begin{bmatrix} 
            1 \\
            1 \\
            \vdots \\
            1
        \end{bmatrix} \tag{4}\label{eq:gamma-theta}. 
\end{equation}\]

<p>Fifthly, $\pmb{\theta}$ is a vector which contains two random variables, $\theta_0$ and $\theta_1$. Specifically, both $\theta_0$ and $\theta_1$ are <strong>Dirichlet</strong> distributions with $\pmb{\gamma_{\theta}}$ as their hyperparameters,</p>

\[\begin{align}
    \theta_0 &amp;\sim \text{Dirichlet}(\pmb{\gamma_{\theta}}) \tag{5}\label{eq:dirichlet-1}, \\
    \theta_1 &amp;\sim \text{Dirichlet}(\pmb{\gamma_{\theta}}) \tag{6}\label{eq:dirichlet-2}.         
\end{align}\]

<p>Last but not least, $\pmb{W}_j$ represents a probability distribution of $j$th document which modeled by a <strong>Multinomial</strong> distribution. As the $j$th document has $R_j$ words and probabilities of each word in the vocabulary, the <strong>Multinomial</strong> distribution is stated as follows:</p>

\[\begin{equation}
    \pmb{W}_j \sim \text{Multinomial}(R_j, \theta_{L_j}), \text{ for }j = 1, \ldots, N. \tag{7}\label{eq:multinomial}
\end{equation}\]

<p>Hopefully, now that we know what those variables are, we can move forward by programming them.</p>

<p>Let’s import all the libraries.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># import all the libraries
</span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">numpy.random</span> <span class="kn">import</span> <span class="n">beta</span><span class="p">,</span> <span class="n">binomial</span><span class="p">,</span> <span class="n">dirichlet</span>
</code></pre></div></div>
<p><br />
Let’s define a function to <em>sample labels</em> for <code class="language-plaintext highlighter-rouge">N</code> documents with hyperparameter $\gamma_{\pi}$ (<code class="language-plaintext highlighter-rouge">gamma_pi</code>). The labels are either <code class="language-plaintext highlighter-rouge">0</code> (<em>negative</em>) or <code class="language-plaintext highlighter-rouge">1</code> (<em>positive</em>); additionally, the number of labels equal to the number of documents.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">sample_labels</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">gamma_pi</span><span class="p">):</span>
    <span class="s">"""Sample labels for N documents according to Beta distribution 
    with a hyperparameter=gamma_pi

    Parameters
    ----------
    N        : int
               Number of documents        
    gamma_pi : list with length=2
               Hyperparameters of the Beta distribution        

    Returns
    -------
    array with shape (N,)
        Labels for the N documents
    """</span>
    <span class="c1"># pi is the Beta distribution with parameters gamma_pi[0] and gamma_pi[1]
</span>    <span class="n">pi</span> <span class="o">=</span> <span class="n">beta</span><span class="p">(</span><span class="n">gamma_pi</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">gamma_pi</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    
    <span class="k">return</span> <span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">pi</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
</code></pre></div></div>
<p>Therefore, we have defined \(L_j\), for \(j=1,2, \ldots, N\) as in the graphical model.</p>

<p>Next, we implement how to generate documents. In generating documents, we employ <em>Poisson distribution</em> to determine the size of a document; in the paper, $R_j$ denotes the size of the $j$th document.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">numpy.random</span> <span class="kn">import</span> <span class="n">multinomial</span><span class="p">,</span> <span class="n">poisson</span>

<span class="k">def</span> <span class="nf">generate_data</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">gamma_pi</span><span class="p">,</span> <span class="n">gamma_theta</span><span class="p">,</span> <span class="n">lmbda</span><span class="p">):</span>
    <span class="s">"""Generate N documents from a hyperparameter of binomial, Dirichlet, and Poisson 

    Parameters
    ----------
    N           : int
                  Number of documents        
    gamma_pi    : list with length=2
                  Hyperparameters of the Beta distribution        
    gamma_theta : list with length=V with V denotes vocabulary size
                  Hyperparameters of the Dirichlet distribution
    lmbda       : int
                  Hyperparameter for Poisson distribution, denotes 
                  number of words in a document
    Returns
    -------
    list of sets with shape (N,)
                  Each set represents a document which contains tuples (i,c)
                  with i denotes word index and c represents frequency of the word index i
    list of integer with shape (N,)
                  Labels of documents  
    """</span>    
    <span class="c1"># Sample N labels for N documents
</span>    <span class="n">labels</span> <span class="o">=</span> <span class="n">sample_labels</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">gamma_pi</span><span class="p">)</span>

    <span class="c1"># Construct two Dirichlet distributions; each has gamma_theta as hyperparameters
</span>    <span class="n">theta</span> <span class="o">=</span> <span class="n">dirichlet</span><span class="p">(</span><span class="n">gamma_theta</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

    <span class="c1"># Initialize a list which contains N documents
</span>    <span class="n">W</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">labels</span><span class="p">:</span>
        <span class="n">R</span> <span class="o">=</span> <span class="n">poisson</span><span class="p">(</span><span class="n">lmbda</span><span class="p">)</span>  <span class="c1"># Sample a size of a document 
</span>        <span class="n">doc</span> <span class="o">=</span> <span class="n">multinomial</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">theta</span><span class="p">[</span><span class="n">l</span><span class="p">])</span> <span class="c1"># Generate a document as a multinomial distribution 
</span>        <span class="c1"># put the document into W
</span>        <span class="c1"># i = word index and c = frequency of the index i  
</span>        <span class="n">W</span><span class="p">.</span><span class="n">append</span><span class="p">({(</span><span class="n">i</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">c</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span> <span class="k">if</span> <span class="n">c</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">})</span>   
    <span class="k">return</span> <span class="n">W</span><span class="p">,</span> <span class="n">labels</span>
</code></pre></div></div>
<p>Basically, <code class="language-plaintext highlighter-rouge">W</code> contains <code class="language-plaintext highlighter-rouge">N</code> documents and each document consists of tuples and a tuple represents a word index and its frequency. For example, we have <code class="language-plaintext highlighter-rouge">W</code> as follows:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">W</span> <span class="o">=</span> <span class="p">[{(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)},</span>
  <span class="p">{(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)}]</span>
</code></pre></div></div>
<p>This <code class="language-plaintext highlighter-rouge">W</code> represents $2$ documents. The first document contains word indices <code class="language-plaintext highlighter-rouge">0</code>, <code class="language-plaintext highlighter-rouge">1</code>, <code class="language-plaintext highlighter-rouge">2</code>, <code class="language-plaintext highlighter-rouge">3</code>, <code class="language-plaintext highlighter-rouge">4</code> with their frequencies <code class="language-plaintext highlighter-rouge">1</code>, <code class="language-plaintext highlighter-rouge">2</code>, <code class="language-plaintext highlighter-rouge">2</code>, <code class="language-plaintext highlighter-rouge">3</code>, <code class="language-plaintext highlighter-rouge">2</code> respectively. The second document contains word indices <code class="language-plaintext highlighter-rouge">0</code>, <code class="language-plaintext highlighter-rouge">1</code>, <code class="language-plaintext highlighter-rouge">2</code>, <code class="language-plaintext highlighter-rouge">3</code>, <code class="language-plaintext highlighter-rouge">4</code> with the frequencies <code class="language-plaintext highlighter-rouge">1</code>, <code class="language-plaintext highlighter-rouge">1</code>, <code class="language-plaintext highlighter-rouge">1</code>, <code class="language-plaintext highlighter-rouge">2</code>, <code class="language-plaintext highlighter-rouge">2</code> respectively.</p>

<p>Finally, we define <code class="language-plaintext highlighter-rouge">initialize</code> method as follows:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">initialize</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">gamma_pi</span><span class="p">,</span> <span class="n">gamma_theta</span><span class="p">):</span>
    <span class="s">"""Initialize all random variables and put them all into initial state 

    Parameters
    ----------
    W           : list of sets 
                  List of documents
    labels      : array with shape (N,)
                  Labels for the N documents   
    gamma_pi    : list with length=2
                  Hyperparameters of the Beta distribution        
    gamma_theta : list with length=V with V denotes vocabulary size
                  Hyperparameters of the Dirichlet distribution
    lmbda       : int
                  Hyperparameter for Poisson distribution, denotes 
                  number of words in a document
    Returns
    -------
    A dictionary
                  A dictionary contains
                    C : an array with size (2,) that contains number of negative documents at index 0 and number of positive documents at index 1 

                    N : an array with size (2, V) that contains frequencies of each word index in negative documents and frequencies of each word index in positive documents
    """</span>       
    <span class="c1"># N is total number of documents  
</span>    <span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">W</span><span class="p">)</span>

    <span class="c1"># M is total number of labels which have been observed
</span>    <span class="n">M</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>

    <span class="c1"># V is size of vocabulary
</span>    <span class="n">V</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">gamma_theta</span><span class="p">)</span>

    <span class="c1"># Only sample the unobserved instances (N-M)
</span>    <span class="n">L</span> <span class="o">=</span> <span class="n">sample_labels</span><span class="p">(</span><span class="n">N</span> <span class="o">-</span> <span class="n">M</span><span class="p">,</span> <span class="n">gamma_pi</span><span class="p">)</span> 

    <span class="c1"># Sample two Dirichlet distributions for each label (0 and 1)
</span>    <span class="n">theta</span> <span class="o">=</span> <span class="n">dirichlet</span><span class="p">(</span><span class="n">gamma_theta</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

    <span class="c1"># Initialize to record number of documents in each labels
</span>    <span class="n">C</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">2</span><span class="p">,))</span>

    <span class="c1"># Add pseudcounts into observed evidence    
</span>    <span class="n">C</span> <span class="o">+=</span> <span class="n">gamma_pi</span>

    <span class="c1"># Initialize cnts; cnts to record frequencies of word indices for each label 
</span>    <span class="n">cnts</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="n">V</span><span class="p">))</span>

    <span class="c1"># Add pseudcounts into observed evidence   
</span>    <span class="n">cnts</span> <span class="o">+=</span> <span class="n">gamma_theta</span>
    
    <span class="c1"># Populate C and cnts 
</span>    <span class="k">for</span> <span class="n">d</span><span class="p">,</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">labels</span><span class="p">.</span><span class="n">tolist</span><span class="p">()</span> <span class="o">+</span> <span class="n">L</span><span class="p">.</span><span class="n">tolist</span><span class="p">()):</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">d</span><span class="p">:</span> 
            <span class="n">cnts</span><span class="p">[</span><span class="n">l</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">c</span>
        <span class="n">C</span><span class="p">[</span><span class="n">l</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">return</span> <span class="p">{</span><span class="s">'C'</span><span class="p">:</span><span class="n">C</span><span class="p">,</span> <span class="s">'N'</span><span class="p">:</span><span class="n">cnts</span><span class="p">,</span> <span class="s">'L'</span><span class="p">:</span><span class="n">L</span><span class="p">,</span> <span class="s">'theta'</span><span class="p">:</span><span class="n">theta</span><span class="p">}</span>    
</code></pre></div></div>
<p>Variables <code class="language-plaintext highlighter-rouge">C</code> and <code class="language-plaintext highlighter-rouge">cnts</code> denotes \(\begin{bmatrix} C_0 + \gamma_{\pi 0} \\ C_1 + \gamma_{\pi 1} \end{bmatrix}\) and \(\begin{bmatrix} 
\pmb{\mathcal{N}_{\mathbb{C}_0}} + \pmb{\gamma_{\theta 0}}  \\ 
\pmb{\mathcal{N}_{\mathbb{C}_1}} + \pmb{\gamma_{\theta 1}}  
\end{bmatrix}\) in the paper respectively. Particularly, \(\pmb{\mathcal{N}_{\mathbb{C}_0}}\), \(\pmb{\mathcal{N}_{\mathbb{C}_1}}\), \(\pmb{\gamma_{\theta 0}}\), and \(\pmb{\gamma_{\theta 1}}\) are vectors with the size \(1 \times V\).</p>

<p>We have reached the end of the first part of <em>Gibbs Sampling for the Uninitiated part 1</em>. In part 2, we shall discuss the building of Gibbs sampler, specifically, how to implement Equation ($49$) on page $16$ of the paper:</p>

\[\begin{equation}
        \text{Pr(L}_j = x | \mathbf{L}^{(-j)}, \mathbb{C}^{(-j)}, \mathbf{\theta_0}, \mathbf{\theta_1}; \mathbf{\mu}) = \frac{C_x + \gamma_{\pi x} - 1}{N + \gamma_{\pi 1} + \gamma_{\pi 0} - 1} \prod_{i=1}^{V}{\theta_{x,i}^{\text{W}_{ji}}}.\tag{8}\label{eq:update-equation}
    \end{equation}\]

<p>The playground of this tutorial is also provided as <em>Jupyter notebook</em> in this <a href="https://github.com/hbunyamin/gibbs-sampling-for-uninitiated-with-python"><strong>repo</strong></a>.</p>

            </div>
            <hr>
            <div class="date">
              Written on July  4, 2020
            </div>
            <br>
            
            
            
<div class="comments">
	<div id="disqus_thread"></div>
	<script type="text/javascript">

	    var disqus_shortname = 'hbunyamin-github-io';

	    (function() {
	        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
	        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	    })();

	</script>
	<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>


      </div>
  </div>


        </div>
      </div>
    </div>
    <script src="/assets/twitter.js"></script>
    
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript" async
  src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

    
	<!-- Google Analytics -->
	<script>
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

		ga('create', 'UA-27831864-1', 'auto');
		ga('send', 'pageview', {
		  'page': '/ml-2/Gibbs_Sampling_for_the_Uninitiated_part_1/',
		  'title': 'Gibbs Sampling for the Uninitiated part 1'
		});
	</script>
	<!-- End Google Analytics -->


  </body>
</html>
