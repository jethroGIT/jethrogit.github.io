<!DOCTYPE html>
<html>
  <head>
      <title>A Foundation of Faster Optimization Algorithms part 1 – Jethro Andersson Apriliano Ofe – Forgiven sinner and Student at Maranatha Christian University</title>

        <meta charset="utf-8" />
    <meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>

    
    <meta name="description" content="This article summarizes the topic “Exponentially Weighted Moving Averages” (Week 2 of Improving Deep Neural Networks: Hyperparameters tuning, Regularization and Optimization) from deeplearning.ai.

There are a few optimization algorithms which are faster than gradient descent. In order to understand these optimization algorithms, we need to understand the concept of exponentially weighted moving averages.
" />
    <meta property="og:description" content="This article summarizes the topic “Exponentially Weighted Moving Averages” (Week 2 of Improving Deep Neural Networks: Hyperparameters tuning, Regularization and Optimization) from deeplearning.ai.

There are a few optimization algorithms which are faster than gradient descent. In order to understand these optimization algorithms, we need to understand the concept of exponentially weighted moving averages.
" />
    
    <meta name="author" content="Jethro Andersson Apriliano Ofe" />

    
    <meta property="og:title" content="A Foundation of Faster Optimization Algorithms part 1" />
    <meta property="twitter:title" content="A Foundation of Faster Optimization Algorithms part 1" />
    

    <!--[if lt IE 9]>
      <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <link rel="icon" href="/favicon.png" type="image/gif">

    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="alternate" type="application/rss+xml" title="Jethro Andersson Apriliano Ofe - Forgiven sinner and Student at Maranatha Christian University" href="/feed.xml" />

    <!-- Created with Jekyll Now - http://github.com/barryclark/jekyll-now -->
  </head>

  <body>

    <div class="vertical-center"> 
      <div class="container">
        <div class="row">
              <div id="left-bar">
                
                  <div class="col-lg-offset-1 col-lg-3 col-md-12 col-sm-12 col-xs-12 left-bar border-right">

    
        <a href="http://localhost:4000/">
            <img src="/assets/images/me-gullit-model.jpg" class="img-responsive hidden-xs hidden-sm hidden-md" alt=""/>
        </a>
    
    <h3 class="text-center">Jethro Andersson Apriliano Ofe <small></small></h3>
<!--    <p class="text-center" style="color: #707073;">Forgiven sinner and Student at Maranatha Christian University</p> -->
        <p class="text-center" style="color: #707073;"><a href="https://jethrogit.github.io/sermons/Forgive_Sinner/"><b>A Forgiven Sinner</b></a> and <a href="http://it.maranatha.edu/resume/hendra-bunyamin/"><b>Student at Maranatha Christian University</b></a></p> 
    <div class="hidden-lg" style="margin-bottom:5px;">
        <p class="text-center">
            <a href="http://localhost:4000" class="color-icon" title="Home"><i class="fa fa-home" aria-hidden="true"></i></a> - 
            
<a href="mailto:jethro.andersson02@gmail.com" class="color-icon"><i class="fa fa-envelope" title="Email"></i></a>


<a href="https://github.com/jethroGIT" class="color-icon" title="Github"><i class="fa fa-github"></i></a>

<a href="https://www.linkedin.com/in/jethro-andersson-apriliano-ofe" class="color-icon" title="Linkedin"><i class="fa fa-linkedin"></i></a>



<!-- <a href="http://stackoverflow.com/users/1473726" class="color-icon" title="StacKoverflow"><i class="fa fa-stack-overflow"></i></a> -->




        </p>
    </div>
    <div class="hidden-xs hidden-sm hidden-md" style="margin-bottom:5px;">
        <hr >

        
        <i class="fa fa-github fw"></i><a href="https://github.com/jethroGIT"> github.com/jethroGIT</a>    <br />
         

        

        

        
        <i class="fa fa-envelope fw"></i><a href="jethro.andersson02@gmail.com" target="_blank"> jethro.andersson02@gmail.com</a>
        

        

        <br>

        
        <i class="fa fa-linkedin fw"></i><a href="https://www.linkedin.com/in/jethro-andersson-apriliano-ofe" target="_blank"> jethro-andersson-apriliano-ofe</a>
        

        

        

        

<!--        
        <hr >
        <a href="https://stackoverflow.com/users/1473726?tab=profile">
           <img src="https://stackoverflow.com/users/flair/1473726.png?theme=clean" width="208" height="58" alt="profile for  at Stack Overflow, Q&amp;A for professional and enthusiast programmers" title="profile for  at Stack Overflow, Q&amp;A for professional and enthusiast programmers">
        </a>
        
        <hr >
        
        <p>You may also want to check this profile:
            <ul>
                
                  <li><a href=""></a></li>
                
            </ul>
        </p>
        
-->        
    </div>

</div>

                
              </div>
              <div class=" col-lg-8   col-sm-12 col-xs-12">
  <div class="row grid-content"> 
      <div class="col-md-12 col-xs-12 projet" style="margin-bottom:20px;">
          <p class="hidden-xs hidden-sm"><a href="http://localhost:4000/">Home</a> <span style="color:darkgrey"> / <a href="http://localhost:4000/computer-vision">computer-vision</a> / a foundation of faster optimization algorithms part 1</span></p>
          <h3>A Foundation of Faster Optimization Algorithms part 1<small class="pull-right"><a href="/feed.xml" target="_blank"><i class="fa fa-rss-square rss"></i></a></small><small class="pull-right"><a class="twitter-share-button" href="https://twitter.com/intent/tweet?via=&text=Exponentially weighted averages">Tweet</a></small></h3> 
          <!-- <hr>
              <p class="text-center">
               A Foundation of Faster Optimization Algorithms part 1 <br> 
                  <span class="text-small">Exponentially weighted averages</span> <br>
                  

              </p>
              <hr> -->

            <div class="entry">
              <p>This article summarizes the topic “<em><strong>Exponentially Weighted Moving Averages</strong></em>” (<a href="https://www.coursera.org/learn/deep-neural-network/home/week/2">Week 2 of <em>Improving Deep Neural Networks: Hyperparameters tuning, Regularization and Optimization</em></a>) from <a href="https://www.coursera.org/specializations/deep-learning?">deeplearning.ai</a>.</p>

<p>There are a few optimization algorithms which are faster than <em>gradient descent</em>. In order to understand these optimization algorithms, we need to understand the concept of <strong>exponentially weighted moving averages</strong>.</p>

<p><a href="/assets/images/temperature-and-days.png"><img src="/assets/images/temperature-and-days.png" alt="img1" class="img-responsive" /></a><em><center>$\pmb{\text{Figure 1}}$: A plot of temperatures for each day in a year. Image taken from <a href="https://www.coursera.org/learn/deep-neural-network/lecture/duStO/exponentially-weighted-averages">Deeplearning.ai</a>, some rights reserved.</center></em>
<br /></p>

<p>Suppose that the temperature of day $i$ is denoted by $\theta_i$ from $i=1, \ldots, 365$ (how many days in one year?). Visually, the temperatures are shown in $\text{Figure 1}$. Furthermore, we show several examples of the temperatures as follows:</p>

\[\begin{align}
	\theta_1 &amp;= 40^{\circ} \text{F} \\
	\theta_2 &amp;= 49^{\circ} \text{F} \\
	\theta_3 &amp;= 40^{\circ} \text{F} \\	
	         &amp;\vdots \\
	\theta_{180} &amp;= 60^{\circ} \text{F} \\		         
	\theta_{181} &amp;= 56^{\circ} \text{F} \\		      
			&amp;\vdots
\end{align}\]

<p>As we can see in the image above, the values of temperatures during a year are <em>noisy</em> which means that there is considerable variation in the values. The variation is caused by <em>noise</em> and <em>we need to remove the variation</em> if we want to expose the underlying values of temperatures (Brownlee, 2019).</p>

<blockquote>
  <p>How do we remove the noise which resides in the values of time series?</p>
</blockquote>

<p>One of the techniques to remove the noise is called <strong>smoothing</strong>. Particularly, the common technique used commonly in time series forecasting is <strong>exponentially weighted averages</strong>. Computing <strong>exponentially weighted averages</strong> involves constructing a new series whose values are calculated by the average of raw observations in the original time series. Let’s denote the new series as $v_t$ for $t=1, 2, \ldots, 365$ as follows:</p>

\[\begin{equation}
 		v_t = \beta v_{t-1} + (1-\beta) \theta_t \tag{1}\label{eq:weighted-averages}. 
	\end{equation}\]

<p>with $v_t$ is the average at time $t$, $\theta_t$ is the temperature at time $t$, and $\beta$ is the parameter determining <em>average number of days’ temperatures</em> ($0 &lt; \beta &lt; 1$). Specifically,</p>

\[\begin{equation}
v_t \approx \pmb{\text{average }} \text{over } \frac{1}{1-\beta} \text{ days' temperature} \tag{2}\label{eq:parameter-beta}. 
\end{equation}\]

<p>For example, let’s define $\beta = 0.9$ which means that we compute $v_t$ as approximately an average over</p>

\[\begin{align}
	\frac{1}{1-\beta} &amp;= \frac{1}{1-0.9} \\
	                  &amp;= \frac{1}{0.1} \\
	                  &amp;= 10 \text{ days}.
\end{align}\]

<p>Similarly, $\beta = 0.98$ gives approximately an average over $50 \text{ days}$.
$\text{Figure 2}$ shows the plot of these weighted averages.</p>

<p><a href="/assets/images/moving-average-at-beta-0.9-and-0.98.png"><img src="/assets/images/moving-average-at-beta-0.9-and-0.98.png" alt="img1" class="img-responsive" /></a><em><center>$\pmb{\text{Figure 2}}$: A plot of $v_t$ at $\beta=0.9$ ($\pmb{\text{red}}$) and $\beta=0.98$ ($\pmb{\text{green}}$). Image taken from <a href="https://www.coursera.org/learn/deep-neural-network/lecture/duStO/exponentially-weighted-averages">Deeplearning.ai</a>, some rights reserved.</center></em>
<br /></p>

<p>As an extreme example, $\beta = 0.5$ computes approximately an average over $2 \text{ days}$ as depicted in $\text{Figure 3}$. These weighted averages are noisy because the computation of these averages takes only $2 \text{ days}$ before.</p>

<p><a href="/assets/images/moving-average-at-beta-0.5.png"><img src="/assets/images/moving-average-at-beta-0.5.png" alt="img1" class="img-responsive" /></a><em><center>$\pmb{\text{Figure 3}}$: A plot of $v_t$ at $\beta=0.5$ ($\pmb{\text{yellow}}$). Image taken from <a href="https://www.coursera.org/learn/deep-neural-network/lecture/duStO/exponentially-weighted-averages">Deeplearning.ai</a>, some rights reserved.</center></em>
<br /></p>

<p>From those three different values of $\beta$, we can see that as $\beta$ is getting larger and larger, the plot is going smoother and smoother. On the other hand, if $\beta$ is getting smaller and smaller, the plot will become noisier and noisier. As a conclusion,</p>

<p>\(\begin{align}
	\beta \ggg \; &amp;\Longrightarrow \pmb{ \text{ smoother}}  \text{ because we are averaging more days} \\
	\beta \lll \; &amp;\Longrightarrow \pmb{ \text{ noisier}} \text{ because we are averaging fewer days}.
\end{align}\) <br />
<br /></p>
<h4 id="bias-correction"><strong>Bias Correction</strong></h4>
<p>Previously, we have defined Equation \eqref{eq:weighted-averages} which is</p>

\[\begin{equation}
 		v_t = \beta v_{t-1} + (1-\beta) \theta_t. 
	\end{equation}\]

<p>Let’s substitute $v_{0} = 0$ and $\beta = 0.98$ into Equation \eqref{eq:weighted-averages}.  <br />
At $t=1$,</p>

\[\begin{align}
		v_1 = \beta v_{0} + (1-\beta) \theta_1 &amp;\Leftrightarrow v_1 = (0.98) (0) + (1-0.98) \theta_1 \\
                                               &amp;\Leftrightarrow v_1 = 0.02 \; \theta_1 \tag{3} \label{eq:v-1}.
	\end{align}\]

<p>At $t=2$,</p>

\[\begin{align}
		v_2 = \beta v_{1} + (1-\beta) \theta_2 &amp;\Leftrightarrow v_2 = (0.98) (0.02 \; \theta_1)  + (1-0.98) \theta_2 \\
                                               &amp;\Leftrightarrow v_2 = 0.0196 \; \theta_1 + 0.02 \; \theta_2 \tag{4} \label{eq:v-2}.
	\end{align}\]

<p>With the assumption that $\theta_1$, $\theta_2 &gt; 0$, we arrive at</p>

\[\begin{equation}
v_2 \ll \theta_1 \text{ and } v_2 \ll \theta_2
 \end{equation}\]

<p>which means that $v_2$ <em>is not a very good estimate</em> of the first two days’ temperature of the year.</p>

<blockquote>
  <p>How can we improve the estimate of the first two days’ temperature?</p>
</blockquote>

<p>Introducing <strong>bias correction</strong> will improve the estimate. With $\beta = 0.98$,</p>

\[\begin{align}
		v_t &amp;\approx \text{ average over } \frac{1}{1-0.98} \\
			&amp;\approx \text{ average over } 50 \text{ days' temperature};
	\end{align}\]

<p>moreover, at $t=2$, the <strong>bias correction</strong> factor will be</p>

<p>\(\begin{align}
		1 - \beta^t &amp;= 1 - (0.98)^2 \\
		            &amp;= 0.0396 \tag{5} \label{eq:bias-correction}
	\end{align}\)
Therefore, the <strong>bias correction</strong> for $v_2$ will be</p>

\[\begin{align}
		\frac{v_2}{1-\beta^2} &amp;= \frac{v_2}{0.0396} &amp; \text{ from Equation }\eqref{eq:bias-correction}.
	\end{align}\]

<p>Since $v_2$ is divided by $0.0396$ which acts as a <strong>bias correction</strong>, the bias from $v_2$ will be removed and the estimate of $v_2$ is improving. Generally, the form of <em><strong>weighted averages with bias correction</strong></em> is defined as</p>

<p>\(\begin{equation}
		\frac{v_t}{1 - \beta^t} = \beta \; v_{t-1} + (1 - \beta) \; \theta_t. \tag{6} \label{eq:with-bias-correction}
	\end{equation}\)
with $t=1,2, \ldots, 365$.</p>

<p>As a final note, the <strong>bias correction</strong> <em>helps us find better estimates of a series during the initial phase of learning</em> as we will see in the Part 2 of this blog. As $t$ is getting larger and larger, $\beta^t \approx 0$. Therefore, if $t$ is large, the <strong>bias correction</strong> will have no effect on the series.  <br />
<br /></p>
<h4 id="references"><strong>References</strong></h4>
<p>Brownlee, J. (2019). <em>Introduction to Time Series Forecasting in Python</em>. <code class="language-plaintext highlighter-rouge">https://machinelearningmastery.com/introduction-to-time-series-forecasting-with-python/</code>. Accessed: 2020-09-5</p>

            </div>
            <hr>
            <div class="date">
              Written on August 22, 2020
            </div>
            <br>
            
            
            
<div class="comments">
	<div id="disqus_thread"></div>
	<script type="text/javascript">

	    var disqus_shortname = 'hbunyamin-github-io';

	    (function() {
	        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
	        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	    })();

	</script>
	<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>


      </div>
  </div>


        </div>
      </div>
    </div>
    <script src="/assets/twitter.js"></script>
    
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript" async
  src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

    
	<!-- Google Analytics -->
	<script>
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

		ga('create', 'UA-27831864-1', 'auto');
		ga('send', 'pageview', {
		  'page': '/computer-vision/Exponentially_Weighted_Averages/',
		  'title': 'A Foundation of Faster Optimization Algorithms part 1'
		});
	</script>
	<!-- End Google Analytics -->


  </body>
</html>
