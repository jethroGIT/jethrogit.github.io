<!DOCTYPE html>
<html>
  <head>
      <title>Gradients of Softmax Output Layer in Gory Details – Jethro Andersson Apriliano Ofe – Forgiven sinner and Student at Maranatha Christian University</title>

        <meta charset="utf-8" />
    <meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>

    
    <meta name="description" content="This article attempts to find gradients of a softmax output layer. This knowledge proves useful when we want to utilize backpropagation algorithm to compute gradients of neural networks with a softmax output layer. Furthermore, page 3 from the outstanding Notes on Backpropagation by Peter Sadowski has inspired this article a lot.
" />
    <meta property="og:description" content="This article attempts to find gradients of a softmax output layer. This knowledge proves useful when we want to utilize backpropagation algorithm to compute gradients of neural networks with a softmax output layer. Furthermore, page 3 from the outstanding Notes on Backpropagation by Peter Sadowski has inspired this article a lot.
" />
    
    <meta name="author" content="Jethro Andersson Apriliano Ofe" />

    
    <meta property="og:title" content="Gradients of Softmax Output Layer in Gory Details" />
    <meta property="twitter:title" content="Gradients of Softmax Output Layer in Gory Details" />
    

    <!--[if lt IE 9]>
      <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <link rel="icon" href="/favicon.png" type="image/gif">

    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="alternate" type="application/rss+xml" title="Jethro Andersson Apriliano Ofe - Forgiven sinner and Student at Maranatha Christian University" href="/feed.xml" />

    <!-- Created with Jekyll Now - http://github.com/barryclark/jekyll-now -->
  </head>

  <body>

    <div class="vertical-center"> 
      <div class="container">
        <div class="row">
              <div id="left-bar">
                
                  <div class="col-lg-offset-1 col-lg-3 col-md-12 col-sm-12 col-xs-12 left-bar border-right">

    
        <a href="http://localhost:4000/">
            <img src="/assets/images/me-gullit-model.jpg" class="img-responsive hidden-xs hidden-sm hidden-md" alt=""/>
        </a>
    
    <h3 class="text-center">Jethro Andersson Apriliano Ofe <small></small></h3>
<!--    <p class="text-center" style="color: #707073;">Forgiven sinner and Student at Maranatha Christian University</p> -->
        <p class="text-center" style="color: #707073;"><a href="https://jethrogit.github.io/sermons/Forgive_Sinner/"><b>A Forgiven Sinner</b></a> and <a href="http://it.maranatha.edu/resume/hendra-bunyamin/"><b>Student at Maranatha Christian University</b></a></p> 
    <div class="hidden-lg" style="margin-bottom:5px;">
        <p class="text-center">
            <a href="http://localhost:4000" class="color-icon" title="Home"><i class="fa fa-home" aria-hidden="true"></i></a> - 
            
<a href="mailto:jethro.andersson02@gmail.com" class="color-icon"><i class="fa fa-envelope" title="Email"></i></a>


<a href="https://github.com/jethroGIT" class="color-icon" title="Github"><i class="fa fa-github"></i></a>

<a href="https://www.linkedin.com/in/jethro-andersson-apriliano-ofe" class="color-icon" title="Linkedin"><i class="fa fa-linkedin"></i></a>



<!-- <a href="http://stackoverflow.com/users/1473726" class="color-icon" title="StacKoverflow"><i class="fa fa-stack-overflow"></i></a> -->




        </p>
    </div>
    <div class="hidden-xs hidden-sm hidden-md" style="margin-bottom:5px;">
        <hr >

        
        <i class="fa fa-github fw"></i><a href="https://github.com/jethroGIT"> github.com/jethroGIT</a>    <br />
         

        

        

        
        <i class="fa fa-envelope fw"></i><a href="jethro.andersson02@gmail.com" target="_blank"> jethro.andersson02@gmail.com</a>
        

        

        <br>

        
        <i class="fa fa-linkedin fw"></i><a href="https://www.linkedin.com/in/jethro-andersson-apriliano-ofe" target="_blank"> jethro-andersson-apriliano-ofe</a>
        

        

        

        

<!--        
        <hr >
        <a href="https://stackoverflow.com/users/1473726?tab=profile">
           <img src="https://stackoverflow.com/users/flair/1473726.png?theme=clean" width="208" height="58" alt="profile for  at Stack Overflow, Q&amp;A for professional and enthusiast programmers" title="profile for  at Stack Overflow, Q&amp;A for professional and enthusiast programmers">
        </a>
        
        <hr >
        
        <p>You may also want to check this profile:
            <ul>
                
                  <li><a href=""></a></li>
                
            </ul>
        </p>
        
-->        
    </div>

</div>

                
              </div>
              <div class=" col-lg-8   col-sm-12 col-xs-12">
  <div class="row grid-content"> 
      <div class="col-md-12 col-xs-12 projet" style="margin-bottom:20px;">
          <p class="hidden-xs hidden-sm"><a href="http://localhost:4000/">Home</a> <span style="color:darkgrey"> / <a href="http://localhost:4000/machine-learning">machine-learning</a> / gradients of softmax output layer in gory details</span></p>
          <h3>Gradients of Softmax Output Layer in Gory Details<small class="pull-right"><a href="/feed.xml" target="_blank"><i class="fa fa-rss-square rss"></i></a></small><small class="pull-right"><a class="twitter-share-button" href="https://twitter.com/intent/tweet?via=&text=The gradients of softmax output layer">Tweet</a></small></h3> 
          <!-- <hr>
              <p class="text-center">
               Gradients of Softmax Output Layer in Gory Details <br> 
                  <span class="text-small">The gradients of softmax output layer</span> <br>
                  

              </p>
              <hr> -->

            <div class="entry">
              <p>This article attempts to find gradients of a <a href="https://en.wikipedia.org/wiki/Softmax_function"><em>softmax</em></a> output layer. This knowledge proves useful when we want to utilize <em>backpropagation algorithm</em> to compute gradients of neural networks with a softmax output layer. Furthermore, <a href="https://drive.google.com/file/d/1UV_psOTNXZ0SB_-varbllZ79dLDSp5qU/view?usp=sharing">page 3 from the outstanding <strong>Notes on Backpropagation</strong> by <em>Peter Sadowski</em></a> has inspired this article a lot.</p>

<p>Suppose that we have a multiclass classification problem with 3 (three) choices that are label $1$, label $2$, and label $3$. The image below shows the very simple artificial neural networks with two layers; particulary, we set the output layer as a softmax output layer.</p>

<p><a href="/assets/images/ann-with-softmax.png"><img src="/assets/images/ann-with-softmax.png" alt="img1" class="img-responsive" /></a></p>

<p>Concretely, we utilize one-hot encoding for the three choices as follows:</p>

<p>\(\begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}\), \(\begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}\), and \(\begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}\) are the representations for label $1$, label $2$, and label $3$ respectively.</p>

<p>Let us define our dataset, \(X = \{ (x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \ldots, (x^{(m)}, y^{(m)}) \}\), which has \(m\) instances and</p>

\[\begin{equation} x^{(i)} = \begin{bmatrix} 1 \\ x_1^{(i)} \end{bmatrix} \text{ and }  y^{(i)} = \begin{bmatrix} y_1^{(i)} \\ y_2^{(i)} \\ y_3^{(i)} \end{bmatrix} \end{equation}\tag{1}\label{eq:label-y}\]

<p>with \(y_1^{(i)}\), \(y_2^{(i)}\), \(y_3^{(i)}\) have only binary values (either 0 or 1) for \(i = 1, 2, \ldots, m\).</p>

<p>We employ <a href="https://en.wikipedia.org/wiki/Softmax_function"><em>softmax</em></a> functions as our predictions. Specifically, we define our first hypoteses</p>

\[\begin{equation} h_1(x^{(i)}) = \frac{\exp{(\Theta_{10} + \Theta_{11} x_1^{(i)}})}{\exp{(\Theta_{10} + \Theta_{11} x_1^{(i)})}+\exp{(\Theta_{20} + \Theta_{21} x_1^{(i)})}+\exp{(\Theta_{30} + \Theta_{31} x_1^{(i)}})}
\end{equation}\tag{2}\label{eq:hyphotesis-1}\]

<p>the second hyphotesis,</p>

\[\begin{equation} h_2(x^{(i)}) = \frac{\exp{(\Theta_{20} + \Theta_{21} x_1^{(i)})}}{\exp{(\Theta_{10} + \Theta_{11} x_1^{(i)})}+\exp{(\Theta_{20} + \Theta_{21} x_1^{(i)})}+\exp{(\Theta_{30} + \Theta_{31} x_1^{(i)}})}
\end{equation}\tag{3}\label{eq:hyphotesis-2}\]

<p>the third hyphotesis,</p>

\[\begin{equation} h_3(x^{(i)}) = \frac{\exp{(\Theta_{30} + \Theta_{31} x_1^{(i)})}}{\exp{(\Theta_{10} + \Theta_{11} x_1^{(i)})}+\exp{(\Theta_{20} + \Theta_{21} x_1^{(i)})}+\exp{(\Theta_{30} + \Theta_{31} x_1^{(i)}})}
\end{equation}\tag{4}\label{eq:hyphotesis-4}\]

<p>and the cost function,</p>

\[\begin{equation} J(\Theta) = - \sum_{i=1}^{m} ( y_1^{(i)} \log h_1(x^{(i)}) + y_2^{(i)} \log h_2(x^{(i)}) + y_3^{(i)} \log h_3(x^{(i)}) )
\end{equation}\tag{5}\label{eq:cost-function}\]

<p>with</p>

\[\begin{equation} \Theta = \begin{bmatrix} \Theta_{10} &amp; \Theta_{11} \\
\Theta_{20} &amp; \Theta_{21} \\
 \Theta_{30} &amp; \Theta_{31} 
 \end{bmatrix}.
\end{equation}\tag{6}\label{eq:the-theta}\]

<p>Now we <strong>will show how to derive the gradient for these softmax activation function</strong>. In other words,</p>
<blockquote>
  <p>What are \(\frac{\partial J}{\partial \Theta_{10}}\), \(\frac{\partial J}{\partial \Theta_{11}}\), \(\frac{\partial J}{\partial \Theta_{20}}\), \(\frac{\partial J}{\partial \Theta_{21}}\), \(\frac{\partial J}{\partial \Theta_{30}}\), and \(\frac{\partial J}{\partial \Theta_{31}}\)?</p>
</blockquote>

<p>Firstly, We show how to derive \(\frac{\partial J}{\partial \Theta_{10}}\) and \(\frac{\partial J}{\partial \Theta_{11}}\).  <br />
<br /></p>
<h4 id="lets-derive-fracpmbpartial-jpmbpartial-theta_10"><strong>Let’s derive \(\frac{\pmb{\partial J}}{\pmb{\partial \Theta_{10}}}\)</strong></h4>
<p>By employing <a href="https://www.khanacademy.org/math/multivariable-calculus"><em>Multivariable Calculus</em></a>, we obtain</p>

\[\begin{equation} \frac{\partial J}{\partial \Theta_{10}} = \underbrace{\frac{\partial J}{\partial h_1}\frac{\partial h_1}{\partial \Theta_{10}}}_{\text{Part I}} + \underbrace{\frac{\partial J}{\partial h_2}\frac{\partial h_2}{\partial \Theta_{10}}}_{\text{Part II}} + \underbrace{\frac{\partial J}{\partial h_3}\frac{\partial h_3}{\partial \Theta_{10}}}_{\text{Part III}}.
\end{equation}\tag{7}\label{eq:gradient-10}\]

<p>The Part I consists of \(\frac{\partial J}{\partial h_1}\frac{\partial h_1}{\partial \Theta_{10}}\). Specifically,</p>

\[\begin{equation} \frac{\partial J}{\partial h_1} = - \sum_{i=1}^{m}{\frac{y_1^{(i)}}{h_1(x^{(i)})}}.
\end{equation}\tag{8}\label{eq:gradient-10-1}\]

<p>By defining</p>

\[\begin{equation} u =  \exp{(\Theta_{10} + \Theta_{11}x_1^{(i)})}
\end{equation}\tag{9}\label{eq:gradient-10-2}\]

<p>and</p>

\[\begin{equation} v =  \exp{(\Theta_{10} + \Theta_{11}x_1^{(i)})} + \exp{(\Theta_{20} + \Theta_{21}x_1^{(i)})} + \exp{(\Theta_{30} + \Theta_{31}x_1^{(i)})}
\end{equation}\tag{10}\label{eq:gradient-10-3}\]

<p>and <a href="https://www.khanacademy.org/math/ap-calculus-ab/ab-differentiation-1-new/ab-2-9/v/quotient-rule">Quotient Rule</a>, we are able to compute \(\frac{\partial h_1}{\partial \Theta_{10}}\) as follows:</p>

\[\begin{align} \frac{\partial h_1}{\partial \Theta_{10}} &amp;= \frac{u^{\prime} v - u v^{\prime}}{v^2} \\
&amp;= \frac{(\exp{(\Theta_{10} + \Theta_{11}x_1^{(i)})})(\exp{(\Theta_{10} + \Theta_{11}x_1^{(i)})} + \exp{(\Theta_{20} + \Theta_{21}x_1^{(i)})} + \exp{(\Theta_{30} + \Theta_{31}x_1^{(i)})}) - (\exp{(\Theta_{10} + \Theta_{11}x_1^{(i)})})^2}{(\exp{(\Theta_{10} + \Theta_{11}x_1^{(i)})} + \exp{(\Theta_{20} + \Theta_{21}x_1^{(i)})} + \exp{(\Theta_{30} + \Theta_{31}x_1^{(i)})})^2} \\
&amp;= \frac{\exp{(\Theta_{10} + \Theta_{11}x_1^{(i)})}}{\exp{(\Theta_{10} + \Theta_{11}x_1^{(i)})} + \exp{(\Theta_{20} + \Theta_{21}x_1^{(i)})} + \exp{(\Theta_{30} + \Theta_{31}x_1^{(i)})}} - \left( \frac{\exp{(\Theta_{10} + \Theta_{11}x_1^{(i)})}}{\exp{(\Theta_{10} + \Theta_{11}x_1^{(i)})} + \exp{(\Theta_{20} + \Theta_{21}x_1^{(i)})} + \exp{(\Theta_{30} + \Theta_{31}x_1^{(i)})}} \right)^2 \\
&amp;= h_1(x^{(i)}) - (h_1(x^{(i)}))^2 \\
&amp;= h_1(x^{(i)}) (1 - h_1(x^{(i)}))\tag{11}\label{eq:gradient-10-4}
\end{align}\]

<p>Finally, we can compute \(\frac{\partial J}{\partial h_1}\frac{\partial h_1}{\partial \Theta_{10}}\) by combining Equation \eqref{eq:gradient-10-1} and Equation \eqref{eq:gradient-10-4} as follows:</p>

\[\require{cancel} \begin{align} \frac{\partial J}{\partial h_1}\frac{\partial h_1}{\partial \Theta_{10}} &amp;= - \sum_{i=1}^{m}{\frac{y_1^{(i)}}{\cancel{h_1(x^{(i)})}} \cancel{h_1(x^{(i)})} (1 - h_1(x^{(i)}) )} \\
      &amp;= - \sum_{i=1}^{m}{y_1^{(i)} (1 - h_1(x^{(i)}))}\tag{12}\label{eq:gradient-10-5}
\end{align}\]

<p>The Part II consists of \(\frac{\partial J}{\partial h_2}\frac{\partial h_2}{\partial \Theta_{10}}\). Specifically,</p>

\[\begin{equation} \frac{\partial J}{\partial h_2} = - \sum_{i=1}^{m}{\frac{y_2^{(i)}}{h_2(x^{(i)})}}.
\end{equation}\tag{13}\label{eq:gradient-10-6}\]

<p>Again, by defining</p>

\[\begin{equation} u =  \exp{(\Theta_{20} + \Theta_{21}x_1^{(i)})}
\end{equation},\tag{14}\label{eq:gradient-10-7}\]

<p>using Equation \eqref{eq:gradient-10-3}, and <a href="https://www.khanacademy.org/math/ap-calculus-ab/ab-differentiation-1-new/ab-2-9/v/quotient-rule">Quotient Rule</a>, we can compute \(\frac{\partial h_2}{\partial \Theta_{10}}\)</p>

\[\begin{align} \frac{\partial h_2}{\partial \Theta_{10}} &amp;= \frac{u^{\prime} v - u v^{\prime}}{v^2} \\
&amp;= \frac{0 - \exp{(\Theta_{20} + \Theta_{21} x_1^{(i)})} \exp{(\Theta_{10} + \Theta_{11} x_1^{(i)})}}{(\exp{(\Theta_{10} + \Theta_{11}x_1^{(i)})} + \exp{(\Theta_{20} + \Theta_{21}x_1^{(i)})} + \exp{(\Theta_{30} + \Theta_{31}x_1^{(i)})})^2} \\
&amp;= - \left(\frac{\exp{(\Theta_{20} + \Theta_{21} x_1^{(i)})}}{\exp{(\Theta_{10} + \Theta_{11}x_1^{(i)})} + \exp{(\Theta_{20} + \Theta_{21}x_1^{(i)})} + \exp{(\Theta_{30} + \Theta_{31}x_1^{(i)})}} \right) \left( \frac{\exp{(\Theta_{10} + \Theta_{11} x_1^{(i)})}}{\exp{(\Theta_{10} + \Theta_{11}x_1^{(i)})} + \exp{(\Theta_{20} + \Theta_{21}x_1^{(i)})} + \exp{(\Theta_{30} + \Theta_{31}x_1^{(i)})}} \right) \\
&amp;= - h_2(x^{(i)}) h_1(x^{(i)}) \tag{15}\label{eq:gradient-10-8}
\end{align}\]

<p>By using Equation \eqref{eq:gradient-10-6} and Equation \eqref{eq:gradient-10-8},  \(\frac{\partial J}{\partial h_2}\frac{\partial h_2}{\partial \Theta_{10}}\) can be computed as</p>

\[\begin{align} \frac{\partial J}{\partial h_2}\frac{\partial h_2}{\partial \Theta_{10}} &amp;= + \sum_{i=1}^{m}{\frac{y_2^{(i)}}{\cancel{h_2(x^{(i)})}} \cancel{h_2(x^{(i)})} h_1(x^{(i)})} \\
      &amp;= \sum_{i=1}^{m}{y_2^{(i)} h_1(x^{(i)})}.\tag{16}\label{eq:gradient-10-9}
\end{align}\]

<p>Lastly, the Part III consists of \(\frac{\partial J}{\partial h_3}\frac{\partial h_3}{\partial \Theta_{10}}\).</p>

<p>Particularly,</p>

\[\begin{equation} \frac{\partial J}{\partial h_3} = - \sum_{i=1}^{m}{\frac{y_3^{(i)}}{h_3(x^{(i)})}}.
\end{equation}\tag{17}\label{eq:gradient-10-10}\]

<p>Again, by defining</p>

\[\begin{equation} u =  \exp{(\Theta_{30} + \Theta_{31}x_1^{(i)})}
\end{equation},\tag{18}\label{eq:gradient-10-11}\]

<p>using Equation \eqref{eq:gradient-10-3}, and <a href="https://www.khanacademy.org/math/ap-calculus-ab/ab-differentiation-1-new/ab-2-9/v/quotient-rule">Quotient Rule</a>, we can compute \(\frac{\partial h_3}{\partial \Theta_{10}}\)</p>

\[\begin{align} \frac{\partial h_3}{\partial \Theta_{10}} &amp;= \frac{u^{\prime} v - u v^{\prime}}{v^2} \\
&amp;= \frac{0 - \exp{(\Theta_{30} + \Theta_{31} x_1^{(i)})} \exp{(\Theta_{10} + \Theta_{11} x_1^{(i)})}}{(\exp{(\Theta_{10} + \Theta_{11}x_1^{(i)})} + \exp{(\Theta_{20} + \Theta_{21}x_1^{(i)})} + \exp{(\Theta_{30} + \Theta_{31}x_1^{(i)})})^2} \\
&amp;= - \left(\frac{\exp{(\Theta_{30} + \Theta_{31} x_1^{(i)})}}{\exp{(\Theta_{10} + \Theta_{11}x_1^{(i)})} + \exp{(\Theta_{20} + \Theta_{21}x_1^{(i)})} + \exp{(\Theta_{30} + \Theta_{31}x_1^{(i)})}} \right) \left( \frac{\exp{(\Theta_{10} + \Theta_{11} x_1^{(i)})}}{\exp{(\Theta_{10} + \Theta_{11}x_1^{(i)})} + \exp{(\Theta_{20} + \Theta_{21}x_1^{(i)})} + \exp{(\Theta_{30} + \Theta_{31}x_1^{(i)})}} \right) \\
&amp;= - h_3(x^{(i)}) h_1(x^{(i)}) \tag{19}\label{eq:gradient-10-12}
\end{align}\]

<p>Again by using Equation \eqref{eq:gradient-10-10} and Equation \eqref{eq:gradient-10-12},  \(\frac{\partial J}{\partial h_3}\frac{\partial h_3}{\partial \Theta_{10}}\) can be computed as</p>

\[\begin{align} \frac{\partial J}{\partial h_3}\frac{\partial h_3}{\partial \Theta_{10}} &amp;= + \sum_{i=1}^{m}{\frac{y_3^{(i)}}{\cancel{h_3(x^{(i)})}} \cancel{h_3(x^{(i)})} h_1(x^{(i)})} \\
      &amp;= \sum_{i=1}^{m}{y_3^{(i)} h_1(x^{(i)})}.\tag{20}\label{eq:gradient-10-13}
\end{align}\]

<p>Finally, combining Equation \eqref{eq:gradient-10-5}, \eqref{eq:gradient-10-9}, and \eqref{eq:gradient-10-13} we obtain</p>

\[\begin{align} \frac{\partial J}{\partial \Theta_{10}} &amp;= \frac{\partial J}{\partial h_1}\frac{\partial h_1}{\partial \Theta_{10}} + \frac{\partial J}{\partial h_2}\frac{\partial h_2}{\partial \Theta_{10}} + \frac{\partial J}{\partial h_3}\frac{\partial h_3}{\partial \Theta_{10}} \\
    &amp;= \sum_{i=1}^{m}{\left( -y_1^{(i)} + y_1^{(i)} h_1(x^{(i)}) + y_2^{(i)} h_1(x^{(i)}) + y_3^{(i)} h_1(x^{(i)}) \right)} \\
    &amp;= \sum_{i=1}^{m}{\left( -y_1^{(i)} + h_1(x^{(i)}) \underbrace{(y_1^{(i)} + y_2^{(i)} + y_3^{(i)})}_{\text{equals to }1}  \right)}    \\
    &amp;= \sum_{i=1}^{m}{(h_1(x^{(i)}) - y_1^{(i)})}\tag{21}\label{eq:final-gradient-1}
\end{align}\]

<p>With the same technique, we also obtain
\(\begin{equation} \frac{\partial J}{\partial \Theta_{11}} = \sum_{i=1}^{m}{( h_1(x^{(i)}) - y_1^{(i)} ) x_1^{(i)}}
\end{equation}\tag{22}\label{eq:final-gradient-2}\)</p>

<p>or in general form,</p>

\[\begin{equation} \frac{\partial J}{\partial \Theta_{kj}} = \sum_{i=1}^{m}{( h_k(x^{(i)}) - y_k^{(i)} ) x_j^{(i)}}
\end{equation}\tag{23}\label{eq:final-gradient-3}\]

<p>with \(x_j^{(i)} = 0\) if \(j = 0\). Although the calculation in output layer is different, surprisingly, Equation \eqref{eq:final-gradient-3} is similar to <a href="https://hbunyamin.github.io/machine-learning/Gradient_Descent_for_Logistic_Regression/">gradients of sigmoid output layer</a>. Hence, utilizing <em>softmax</em> output layer should be no worries.</p>


            </div>
            <hr>
            <div class="date">
              Written on May 27, 2020
            </div>
            <br>
            
            
            
<div class="comments">
	<div id="disqus_thread"></div>
	<script type="text/javascript">

	    var disqus_shortname = 'hbunyamin-github-io';

	    (function() {
	        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
	        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	    })();

	</script>
	<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>


      </div>
  </div>


        </div>
      </div>
    </div>
    <script src="/assets/twitter.js"></script>
    
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript" async
  src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

    
	<!-- Google Analytics -->
	<script>
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

		ga('create', 'UA-27831864-1', 'auto');
		ga('send', 'pageview', {
		  'page': '/machine-learning/The_Gradient_of_Softmax/',
		  'title': 'Gradients of Softmax Output Layer in Gory Details'
		});
	</script>
	<!-- End Google Analytics -->


  </body>
</html>
