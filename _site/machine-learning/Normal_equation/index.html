<!DOCTYPE html>
<html>
  <head>
      <title>Deriving Normal Equation of Linear Regression Model – Jethro Andersson Apriliano Ofe – Forgiven sinner and Student at Maranatha Christian University</title>

        <meta charset="utf-8" />
    <meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>

    
    <meta name="description" content="This article is inspired by an excellent post by Eli Bendersky. Let’s continue with the derivation.
" />
    <meta property="og:description" content="This article is inspired by an excellent post by Eli Bendersky. Let’s continue with the derivation.
" />
    
    <meta name="author" content="Jethro Andersson Apriliano Ofe" />

    
    <meta property="og:title" content="Deriving Normal Equation of Linear Regression Model" />
    <meta property="twitter:title" content="Deriving Normal Equation of Linear Regression Model" />
    

    <!--[if lt IE 9]>
      <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <link rel="icon" href="/favicon.png" type="image/gif">

    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="alternate" type="application/rss+xml" title="Jethro Andersson Apriliano Ofe - Forgiven sinner and Student at Maranatha Christian University" href="/feed.xml" />

    <!-- Created with Jekyll Now - http://github.com/barryclark/jekyll-now -->
  </head>

  <body>

    <div class="vertical-center"> 
      <div class="container">
        <div class="row">
              <div id="left-bar">
                
                  <div class="col-lg-offset-1 col-lg-3 col-md-12 col-sm-12 col-xs-12 left-bar border-right">

    
        <a href="http://localhost:4000/">
            <img src="/assets/images/me-gullit-model.jpg" class="img-responsive hidden-xs hidden-sm hidden-md" alt=""/>
        </a>
    
    <h3 class="text-center">Jethro Andersson Apriliano Ofe <small></small></h3>
<!--    <p class="text-center" style="color: #707073;">Forgiven sinner and Student at Maranatha Christian University</p> -->
        <p class="text-center" style="color: #707073;"><a href="https://jethrogit.github.io/sermons/Forgive_Sinner/"><b>A Forgiven Sinner</b></a> and <a href="http://it.maranatha.edu/resume/hendra-bunyamin/"><b>Student at Maranatha Christian University</b></a></p> 
    <div class="hidden-lg" style="margin-bottom:5px;">
        <p class="text-center">
            <a href="http://localhost:4000" class="color-icon" title="Home"><i class="fa fa-home" aria-hidden="true"></i></a> - 
            
<a href="mailto:jethro.andersson02@gmail.com" class="color-icon"><i class="fa fa-envelope" title="Email"></i></a>


<a href="https://github.com/jethroGIT" class="color-icon" title="Github"><i class="fa fa-github"></i></a>

<a href="https://www.linkedin.com/in/jethro-andersson-apriliano-ofe" class="color-icon" title="Linkedin"><i class="fa fa-linkedin"></i></a>



<!-- <a href="http://stackoverflow.com/users/1473726" class="color-icon" title="StacKoverflow"><i class="fa fa-stack-overflow"></i></a> -->




        </p>
    </div>
    <div class="hidden-xs hidden-sm hidden-md" style="margin-bottom:5px;">
        <hr >

        
        <i class="fa fa-github fw"></i><a href="https://github.com/jethroGIT"> github.com/jethroGIT</a>    <br />
         

        

        

        
        <i class="fa fa-envelope fw"></i><a href="jethro.andersson02@gmail.com" target="_blank"> jethro.andersson02@gmail.com</a>
        

        

        <br>

        
        <i class="fa fa-linkedin fw"></i><a href="https://www.linkedin.com/in/jethro-andersson-apriliano-ofe" target="_blank"> jethro-andersson-apriliano-ofe</a>
        

        

        

        

<!--        
        <hr >
        <a href="https://stackoverflow.com/users/1473726?tab=profile">
           <img src="https://stackoverflow.com/users/flair/1473726.png?theme=clean" width="208" height="58" alt="profile for  at Stack Overflow, Q&amp;A for professional and enthusiast programmers" title="profile for  at Stack Overflow, Q&amp;A for professional and enthusiast programmers">
        </a>
        
        <hr >
        
        <p>You may also want to check this profile:
            <ul>
                
                  <li><a href=""></a></li>
                
            </ul>
        </p>
        
-->        
    </div>

</div>

                
              </div>
              <div class=" col-lg-8   col-sm-12 col-xs-12">
  <div class="row grid-content"> 
      <div class="col-md-12 col-xs-12 projet" style="margin-bottom:20px;">
          <p class="hidden-xs hidden-sm"><a href="http://localhost:4000/">Home</a> <span style="color:darkgrey"> / <a href="http://localhost:4000/machine-learning">machine-learning</a> / deriving normal equation of linear regression model</span></p>
          <h3>Deriving Normal Equation of Linear Regression Model<small class="pull-right"><a href="/feed.xml" target="_blank"><i class="fa fa-rss-square rss"></i></a></small><small class="pull-right"><a class="twitter-share-button" href="https://twitter.com/intent/tweet?via=&text=How to derive normal equation">Tweet</a></small></h3> 
          <!-- <hr>
              <p class="text-center">
               Deriving Normal Equation of Linear Regression Model <br> 
                  <span class="text-small">How to derive normal equation</span> <br>
                  

              </p>
              <hr> -->

            <div class="entry">
              <p>This article is inspired by <a href="https://eli.thegreenplace.net/2015/the-normal-equation-and-matrix-calculus">an excellent post by Eli Bendersky</a>. Let’s continue with the derivation.</p>

<p><strong>Cost function</strong> has been explained in <a href="https://www.coursera.org/learn/machine-learning/home/week/1">Week 1</a> and <a href="https://www.coursera.org/learn/machine-learning/home/week/2">Week 2</a> of <a href="https://www.coursera.org/learn/machine-learning/home/welcome"><em>Machine Learning</em> course taught by Andrew Ng</a>. This post tries to explain how to derive <strong>normal equation</strong> for <em>linear regression with multiple variables</em>. It is a good thing if all readers has studied <a href="https://www.coursera.org/learn/machine-learning/home/week/1">Week 1</a> and <a href="https://www.coursera.org/learn/machine-learning/home/week/2">Week 2</a> before reading this post.</p>

<p>The <strong>cost function</strong> of <em>linear regression with multiple variables</em>, $J(\theta)$ is formulated as follows:</p>

\[\begin{equation} J(\theta) = \frac{1}{2m} \sum_{i=1}^{m}{(h_{\theta}(x^{(i)}) - y^{(i)})^2}  \tag{1}\label{eq:cost-function} \end{equation}\]

<p>with $m$ is number of instances in dataset, $h_{\theta}(x^{(i)})$ is our hyphotesis also known as prediction model for the $i$th instance, and $y^{(i)}$ is true value for the $i$th instance.</p>

<p>We also have studied that</p>

\[\begin{equation} h_{\theta}(x^{(i)}) = \theta_0 + \theta_1 x_1^{(i)} + \cdots + \theta_n x_n^{(i)}  \tag{2}\label{eq:the-hyphotesis} \end{equation}\]

<p>By substituting \eqref{eq:the-hyphotesis} into \eqref{eq:cost-function}, we obtain</p>

\[\begin{align}  J(\theta) &amp;= \frac{1}{2m} \sum_{i=1}^{m}{(\theta_0 + \theta_1 x_1^{(i)} + \cdots + \theta_n x_n^{(i)} - y^{(i)})^2}  \tag{3}\label{eq:derivation-1} \\
&amp;= \frac{1}{2m} ((\theta_0 + \theta_1 x_1^{(1)} + \cdots + \theta_n x_n^{(1)} - y^{(1)} )^2 + \cdots + (\theta_0 + \theta_1 x_1^{(m)} + \cdots + \theta_n x_n^{(m)} - y^{(m)} )^2 ) \tag{4}\label{eq:derivation-2} \\
&amp;= \frac{1}{2m} \underbrace{ \begin{bmatrix} (\theta_0 + \theta_1 x_1^{(1)} + \cdots + \theta_n x_n^{(1)} - y^{(1)}) &amp; \cdots &amp; (\theta_0 + \theta_1 x_1^{(m)} + \cdots + \theta_n x_n^{(m)} - y^{(m)}) \end{bmatrix}}_{\text{matrix with size: } 1 \times m} \underbrace{\begin{bmatrix} (\theta_0 +\theta_1 x_1^{(1)} + \cdots + \theta_n x_n^{(1)} - y^{(1)}) \\
\vdots \\
(\theta_0 +\theta_1 x_1^{(m)} + \cdots + \theta_n x_n^{(m)} - y^{(m)})
 \end{bmatrix}}_{\text{matrix with size: } m \times 1} \tag{5}\label{eq:derivation-3} \\
 &amp;= \frac{1}{2m} \left( \begin{bmatrix} (\theta_0 + \theta_1 x_1^{(1)} + \cdots + \theta_n x_n^{(1)} ) &amp; \cdots &amp; (\theta_0 + \theta_1 x_1^{(m)} + \cdots + \theta_n x_n^{(m)} ) \end{bmatrix} - \begin{bmatrix} y^{(1)} &amp; \cdots &amp; y^{(m)} \end{bmatrix} \right) \left( \begin{bmatrix} \theta_0 + \theta_1 x_1^{(1)} + \cdots + \theta_n x_n^{(1)} \\
 \theta_0 + \theta_1 x_1^{(2)} + \cdots + \theta_n x_n^{(2)} \\
 \vdots \\
 \theta_0 + \theta_1 x_1^{(m)} + \cdots + \theta_n x_n^{(m)} \end{bmatrix}  - \begin{bmatrix} y^{(1)} \\ 
 y^{(2)} \\
 \vdots \\
 y^{(m)} \end{bmatrix} \right) \tag{6}\label{eq:derivation-4} \\
 &amp;= \frac{1}{2m} \left( \begin{bmatrix} \theta_0 &amp; \theta_1 &amp; \cdots &amp; \theta_n \end{bmatrix} \begin{bmatrix} 1 &amp; 1 &amp; \cdots &amp; 1 \\ 
 x_1^{(1)} &amp; x_1^{(2)} &amp; \cdots &amp; x_1^{(m)} \\
 \vdots &amp; \vdots &amp; \cdots &amp; \vdots \\
 x_n^{(1)} &amp; x_n^{(2)} &amp; \cdots &amp; x_n^{(m)} \\\end{bmatrix} - \begin{bmatrix} y^{(1)} &amp; \cdots &amp; y^{(m)} \end{bmatrix} \right) \left( \begin{bmatrix} 1 &amp; x_1^{(1)} &amp; \cdots &amp; x_n^{(1)} \\
  1 &amp; x_1^{(2)} &amp; \cdots &amp; x_n^{(2)} \\
  \vdots &amp; \vdots &amp; \cdots &amp; \vdots \\
  1 &amp; x_1^{(m)} &amp; \cdots &amp; x_n^{(m)} \end{bmatrix} \begin{bmatrix} \theta_0 \\ 
 \theta_1 \\
 \vdots \\
 \theta_n \end{bmatrix} - \begin{bmatrix} y^{(1)} \\
  y^{(2)} \\ 
  \ldots \\
  y^{(m)} \end{bmatrix} \right) \tag{7}\label{eq:derivation-5}  \end{align}\]

<p>By defining</p>

\[\begin{equation} \theta = \begin{bmatrix} \theta_0 \\ 
\theta_1 \\ 
\vdots \\
\theta_n \end{bmatrix} \tag{8}\label{eq:defining-theta} \end{equation}\]

<p>and</p>

\[\begin{equation} X = \begin{bmatrix} 1 &amp; x_1^{(1)} &amp; \cdots &amp; x_n^{(1)} \\
1      &amp; x_1^{(2)} &amp; \cdots  &amp; x_n^{(2)} \\ 
\vdots &amp; \vdots    &amp;  \vdots &amp;  \vdots  \\
1 &amp; x_1^{(m)} &amp; \cdots &amp; x_n^{(m)} \end{bmatrix} \tag{9}\label{eq:defining-X} \end{equation}\]

<p>also</p>

\[\begin{equation} Y = \begin{bmatrix} y^{(1)} \\ 
y^{(2)} \\ 
\vdots \\
y^{(m)} \end{bmatrix}, \tag{10}\label{eq:defining-Y} \end{equation}\]

<p>equation \eqref{eq:derivation-5} becomes</p>

\[\begin{align} J(\theta) &amp;= \frac{1}{2m} (\theta^T X^T - Y^T)(X \theta - Y) &amp;&amp;\text{by transpose property} \tag{11}\label{eq:derivation-6} \\
&amp;= \frac{1}{2m} (\theta^T X^T X \theta - \underbrace{\theta^T X^T Y}_{\text{a scalar}} - \underbrace{Y^T X \theta}_{\text{a scalar}} + Y^T Y)  &amp;&amp;\text{by matrix multiplication} \tag{12}\label{eq:derivation-7} \\
&amp;= \frac{1}{2m} (\theta^T X^T X \theta - \theta^T X^T Y - (X \theta)^T Y + Y^T Y) &amp;&amp;\text{by rearranging a scalar} \tag{13}\label{eq:derivation-8} \\
&amp;= \frac{1}{2m} (\theta^T X^T X \theta - \theta^T X^T Y - \theta^T X^T Y + Y^T Y) &amp;&amp;\text{by transpose property} \tag{14}\label{eq:derivation-9} \\
&amp;= \frac{1}{2m} (\underbrace{\theta^T X^T X \theta}_{\text{Part I}} - \underbrace{2 \theta^T X^T Y}_{\text{Part II}} + Y^T Y) &amp;&amp;\text{by summation property} \tag{15}\label{eq:derivation-10} \\
\end{align}\]

<p>We have arrived into a matrix form from <strong>linear regression cost function</strong>. Our next step would be:</p>

<blockquote>
  <p>How can we minimize the <strong>cost function</strong> in Equation \eqref{eq:derivation-10}?</p>
</blockquote>

<p>We will employ the derivation formula from <a href="https://en.wikipedia.org/wiki/Matrix_calculus">Matrix Calculus</a>; specifically, we use <strong>two scalar-by-vector identities</strong> with <strong>denominator layout</strong> (result: column vector). The identities are as follows:</p>

\[\begin{equation} 
	\frac{\partial \mathbf{x}^T \mathbf{A} \mathbf{x} }{\partial \mathbf{x}} = 2 \mathbf{A} \mathbf{x} \tag{16}\label{eq:identity-1}
\end{equation}\]

<p>and</p>

\[\begin{equation} 
	\frac{\partial \mathbf{a}^T \mathbf{x} }{\partial \mathbf{x}} = \frac{\partial \mathbf{x}^T \mathbf{a} }{\partial \mathbf{x}} = \mathbf{a} \tag{17}\label{eq:identity-2}
\end{equation}\]

<p>Now equipped with these identities, let us minimize Equation \eqref{eq:derivation-10} by computing the first derivation of $J(\theta)$; specifically, the Part I is computed with Equation \eqref{eq:identity-1} and Part II with Equation \eqref{eq:identity-2}:</p>

\[\begin{equation} 
	\frac{\partial J }{\partial \theta} = \frac{1}{2m} ( 2 X^T X \theta - 2 X^T Y ) \tag{18}\label{eq:derivation-of-J}
\end{equation}\]

<p>In order to find $\theta$ which minimize Equation \eqref{eq:derivation-10}, we need to solve</p>

\[\begin{align} \frac{\partial J}{\partial \theta} = 0 &amp;\Longleftrightarrow \frac{1}{2m} ( 2 X^T X \theta - 2 X^T Y ) = 0 \\
 &amp;\Longleftrightarrow 2 X^T X \theta - 2 X^T Y = 0 \\
   &amp;\Longleftrightarrow 2 X^T X \theta = 2 X^T Y  \\ 
   &amp;\Longleftrightarrow X^T X \theta = X^T Y \\ 
   &amp;\Longleftrightarrow \theta = (X^T X)^{-1} X^T Y &amp;&amp;\text{by inverse matrix} \tag{19}\label{eq:final-result} \end{align}\]

<p>At last, we have derived <strong>the normal equation of linear regression model</strong> that is</p>

\[\begin{equation} \bbox[5px,border:2px solid blue] {\theta = (X^T X)^{-1} X^T Y}. \end{equation}\]

            </div>
            <hr>
            <div class="date">
              Written on August 18, 2019
            </div>
            <br>
            
            
            
<div class="comments">
	<div id="disqus_thread"></div>
	<script type="text/javascript">

	    var disqus_shortname = 'hbunyamin-github-io';

	    (function() {
	        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
	        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	    })();

	</script>
	<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>


      </div>
  </div>


        </div>
      </div>
    </div>
    <script src="/assets/twitter.js"></script>
    
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript" async
  src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

    
	<!-- Google Analytics -->
	<script>
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

		ga('create', 'UA-27831864-1', 'auto');
		ga('send', 'pageview', {
		  'page': '/machine-learning/Normal_equation/',
		  'title': 'Deriving Normal Equation of Linear Regression Model'
		});
	</script>
	<!-- End Google Analytics -->


  </body>
</html>
